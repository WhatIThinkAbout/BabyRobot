{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[A Baby Robot's Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)__\n",
    "\n",
    "\n",
    "# An Introduction To Reinforcement Learning\n",
    "## Reinforcement Learning: Part 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](GoodImages/header_image.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<font size=\"6\">O</font><font size=\"4\">nce upon a time there was a Baby Robot who got lost in the mall. Using the strategies from Multi-Armed Bandits he was able to recharge, in the shortest time possible, and is now ready to start looking for his mum.\n",
    "\n",
    "Unfortunately he can't quite remember his way back to her, so will need our help to guide him. We'll do this using Reinforcement Learning, to help him find his way and ensure that he gets back safely.</font>\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "The standard reinforcement learning book or syllabus will begin with a description of the full reinforcement learning system and the derivation of the equations used to describe that system. Only then, once they have all the theory in place, will they show how that theory can be applied to practical applications. \n",
    "\n",
    "In this article we'll take the opposite approach. We'll start with very simple methods of practically solving very simple problems and gradually build on these, adding the bits of theory as required, until we're able to solve the full reinforcement learning problem.\n",
    "\n",
    "So we'll end up in the same place as a traditional course, but by using a <i>top-down</i> approach, rather than the standard <i>bottom-up</i> method. We want to get Baby Robot back to his mum as quickly as possible, so don't have time to learn all the theory up-front, instead we'll add it as we go!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Contents\n",
    "\n",
    "In this post we'll cover the following Reinforcement Learning topics:\n",
    "\n",
    "* The  terminology of Reinforcement Learning (RL)\n",
    "* Basic RL mathematics\n",
    "* Policy Evaluation\n",
    "    - Iterative Policy Evaluation\n",
    "* Policy Improvement\n",
    "* Policy Iteration\n",
    "* Value Iteration\n",
    "\n",
    "---\n",
    "\n",
    "![](GoodImages/green_babyrobot_small.gif)\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A note on the code used in this notebook\n",
    "\n",
    "To keep this notebook relatively tidy, all of the main functionality is contained in Python classes, each of which is in its own file in the \"lib\" directory. \n",
    "\n",
    "To get a good understanding of how policy evaluation, policy iteration and value iteration work I'd recommend taking a look at these underlying code files. Particularly examine how the equations contained in this notebook are implemented as iterative updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install the fantastic ipycanvas library to allow drawing to the \n",
    "# browser canvas: https://ipycanvas.readthedocs.io/en/latest/ \n",
    "!pip install -U ipycanvas -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.insert(0, './lib')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from time import sleep, time\n",
    "\n",
    "from ipywidgets import Layout\n",
    "from ipywidgets import Play, IntProgress, HBox, VBox, link\n",
    "\n",
    "from direction import Direction\n",
    "from arrows import Arrows\n",
    "from grid_level import GridLevel, setup_play_level\n",
    "from robot_position import RobotPosition\n",
    "from policy import Policy\n",
    "from policy_evaluation import PolicyEvaluation\n",
    "from value_iteration import ValueIteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_width = 5\n",
    "level_height = 3\n",
    "level = GridLevel( level_width,level_height,start=[0,1],end=[3,2],add_compass=True,fill_center=True )\n",
    "robot_position = RobotPosition(level,initial_sprite = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# The Terminology of Reinforcement Learning (RL)\n",
    "\n",
    "Consider this level where Baby Robot initially finds himself:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "874b945fc4294881a4eb2954d19206c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=424)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On entering the level he has two possible choices: he can either go North or South. Which of these will get him to the exit the quickest?\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Rewards\n",
    "\n",
    "The fundamental concept in Reinforcement Learning (RL) is the concept of a <b><i>reward</i></b>: a single numerical value that is used to measure how well the task at hand has been performed. It is the reward that drives learning, allowing a problem solving strategy to be optimised to give the maximum reward.\n",
    "\n",
    "In problems such as this one, in which Baby Robot must find his way out of a level, a reward could be given simply for reaching the exit, however that doesn't fully describe what we want to achieve. We actually want to get to the exit in the shortest amount of time. Giving a reward just for reaching the exit doesn't encourage this behaviour. Baby Robot could spend days walking around the level before finding the exit and would receive exactly the same amount of reward as he would for proceeding there directly.\n",
    "\n",
    "Therefore a better reward system would be one that encourages taking the shortest route and that discourages wandering around aimlessly. Effectively we want to express the reward as a penalty, that increases with the number of steps taken to reach the exit.\n",
    "\n",
    "However, we also still want to stick with RL's central idea of maximising rewards and so we introduce the idea of negative rewards. For each step taken we give a reward of -1. Therefore a route that takes a long time to find the exit will accumulate a large negative reward and one that goes there directly will have a small negative reward. In terms of getting the most reward, the direct route will be better since it will be less negative.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## States\n",
    "\n",
    "With our reward system, each time Baby Robot moves from one square to the next he will be given a reward of -1. In RL terminology, each of these squares represents a <b><i>State</i></b>, where a state is defined to be a unique, self-contained, stage in the <b><i>Environment</i></b> that we're working in. \n",
    "\n",
    "So, in this case, each state describes a position within the grid that forms the level. In a game, such as chess, the state would describe the current board position. In the case of a self-driving car the state could describe properties such as the position on the road, the direction and speed of the car and details of other traffic. In each case the state defines the current situation.\n",
    "\n",
    "The state is self-contained in the respect that it is independent of any previous states. For example, in chess, the state given by the current board position is independent of all other moves that have been made up to that point. To choose the next move you don't need to know which moves were made in the past. Similarly, for Baby Robot to move from one square to the next in the grid, he doesn't need to know which states he was previously in. When a state is independent of the prior states it is said to satisfy the <b><i>Markov Property</i></b>.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Value\n",
    "\n",
    "Since we know that a penalty (reward) of -1 is incurred each time Baby Robot moves from one state to the next, and we also have the luxury of being able to see the map of the level, we can help Baby Robot to make his choice by calculating the <b><i>value</i></b> of each state, where the value defines how good it is to be in a particular state. \n",
    "\n",
    "Obviously it's better to be in a state that's close to the exit than it is to be in one that's far away. In RL the value of a state is defined to be the expected reward that can be obtained when starting in that state and then proceeding to choose actions that are defined by a plan, or <b><i>policy</i></b>, in all future states.\n",
    "\n",
    "Let's add this information to the level. Starting at the exit (which in RL is referred to as the <b><i>Terminal State</i></b>, where the episode ends and which, by definition, is given a reward value of zero), and working our way around the level, incurring a penalty of -1 each time we move to a new square, gives the following values for each state:  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "20d9154443a44b93be186b46cfb491d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=424)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level_value = np.array([[-5,-6,-5,-4,-3],\n",
    "                        [-4, 0, 0, 0,-2],\n",
    "                        [-3,-2,-1, 0,-1]])\n",
    "\n",
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=True,fill_center=True)\n",
    "robot_position = RobotPosition(level)\n",
    "\n",
    "level.show_values(level_value)\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "There are a few points to note here:\n",
    "\n",
    "\n",
    "* The value of any state is equal to the expected amount of reward that can be accumulated when starting in that state. In RL language the expected total amount of reward is referred to as the return. \n",
    "So, given Baby Robot's starting position, if he is given a reward of -1 each time he moves from one state to the next, he can expect a return of -4 if he follows the shortest path to the exit.\n",
    "\n",
    "\n",
    "* Additionally, the value of a state is given by the sum of taking an action in that state and the value of the next state. For example, moving South from the initial state gives a reward of -1 for taking the action and the new state has a value of -3, therefore the value of the initial state equals -4.\n",
    "\n",
    "\n",
    "* By looking ahead one state we can choose to move in the direction that would take us to a state with a less negative value. So, from the initial position, we'd choose to go South to a state with a value of -3, rather than going North to one with a value of -5. The strategy used to select the next action is known as the <b><i>policy</b></i>. \n",
    "In this case if we choose to move in the direction of increasing value we are actually following the best or <b><i>optimal</b></i> policy and, as we saw for __[Multi-Armed Bandits](https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697)__, when we choose to take the action that gives us the highest amount of immediate reward we are said to be choosing greedily. In this case a greedy policy results in an optimal policy.\n",
    "\n",
    "\n",
    "We can show the policy on our level diagram, where the arrows now point in the direction of the action that should be taken in each state. Note how for this greedy policy the arrows always point in the direction that will give the maximum reward from the current state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0abb4860729b42e19242b7705f85bde9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=424)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=True,fill_center=True)\n",
    "robot_position = RobotPosition(level)\n",
    "\n",
    "policy = Policy(level)\n",
    "directions = policy.get_directions(level_value)\n",
    "\n",
    "level.show_directions(directions)\n",
    "level.show_values(level_value)\n",
    "\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, from Baby Robot's starting position, if he follows this optimal policy he will arrive at the exit with an accumulated total reward of -4.\n",
    "Since we give a fixed reward when moving from one state to the next, under this optimal policy the expected return, and therefore the value of each state, is simply the number of steps to the exit, multiplied by the reward of -1 that is given for each step.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Basic Mathematics\n",
    "\n",
    "In the description of the initial level, where Baby Robot finds himself, we've already covered most of the basic concepts of Reinforcement Learning. We can now add the simple mathematical terms that go with these concepts and then build on these as we go along.\n",
    "\n",
    "Firstly we've said that Baby Robot receives a <i>reward</i> when he takes an action in a state. Unsurprisingly we use the first letter of each of these terms to refer to its corresponding value, with lower-case being used to refer to specific values for each of these quantities, which gives us:\n",
    "\n",
    "* r = reward\n",
    "* a = action\n",
    "* s = state\n",
    "\n",
    "Additionally, when Baby Robot takes an action he'll most likely move from his current state to another state. The next or <i>successor</i> state is denoted by s′ (read as \"s prime\"):\n",
    "\n",
    "* s′ = next state\n",
    "\n",
    "The rewards, states and actions are actually random variables: there's a probability of getting a certain reward, taking a specific action or being in a certain state and these probabilities are referred to using capital letters. \n",
    "\n",
    "Tying all these terms together gives us the expected reward for a state-action pair:\n",
    "\n",
    "\n",
    "![](GoodImages/equation_1.png)\n",
    "\n",
    "\n",
    "<i><center>Equation 1: expected reward for state-action pair</center></i>\n",
    "\n",
    "\n",
    "So, at time 't -1', starting in a particular state 's' and taking action 'a', the expected reward that's received at the next time step is a function of the current state and action. The reason it's an expected reward is because the amount of reward that's received, when repeatedly taking a particular action in a particular state, may not always return a constant value and so this effectively defines the mean value that will be obtained.\n",
    "\n",
    "\n",
    "Using these terms we can create equations for the basic properties we've defined:\n",
    "\n",
    "\n",
    "<b><i>Return 'Gₜ'</b></i>: the total amount of reward accumulated over an episode.\n",
    "In our case an episode refers to all the time steps that occur between entering and exiting a level. (Things obviously get a bit more complicated in long-running or continuous tasks, but we'll come back to that later).\n",
    "\n",
    "\n",
    "![](GoodImages/equation_2.png)\n",
    "\n",
    "<i><center>Equation 2: the return expressed as a sum of rewards.</center></i>\n",
    "    \n",
    "\n",
    "So, starting at time 't', the return is just the sum of the future rewards.\n",
    "\n",
    "\n",
    "<i><b>Value</b>: the value of a state is just a measure of how good it is to be in that state.</i> \n",
    "This can be expressed in terms of the amount of future reward or, in other words, the return that you're likely to receive if you start in that state. Obviously, no matter which state you start in, the rewards you'll receive will depend on the actions you choose and these actions are determined by the <b><i>Policy</b></i>, which is commonly denoted by the symbol 'π'.\n",
    "\n",
    "So the value for state 's' under policy π is simply the expected return:\n",
    "\n",
    "\n",
    "![](GoodImages/equation_3.png)\n",
    "\n",
    "<i><center>Equation 3: the value of state 's' under policy 'π'.</center></i>\n",
    "\n",
    "\n",
    "As we saw, when following a greedy policy, in every state Baby Robot will simply choose to take the action that gives him the highest immediate reward. In most cases this results in a state only having a single possible action. When more than one action exists taking any of the possible actions will result in moving to a state of equal value.\n",
    "\n",
    "Additionally, since in our simple case, we always get the same reward of -1 for taking an action, the value of a state is simply the immediate reward plus the value of the next state:\n",
    "\n",
    "\n",
    "![](GoodImages/equation_4.png)\n",
    "\n",
    "<i><center>Equation 4: The value of state 's' under a deterministic policy 'π'.</center></i>\n",
    "\n",
    "\n",
    "For example, the value of the start state of the level is -4. If the optimal policy is followed from this state Baby Robot will have accumulated a total reward of -4 by the time he reaches the exit. Similarly, if he chooses to take a single step South from the initial state, he'll receive a reward of -1 for taking the action and the value of the next state is -3, so again this give the value of the initial state as -4.\n",
    "\n",
    "Note how the calculation of the state value is split into two parts; the immediate reward achieved by taking the action and the value of the state where that action takes you. This technique of splitting a problem into sub-problems is known as Dynamic Programming. Using this, state values that have already been calculated can be reused when calculating the values of other states whose actions lead there. \n",
    "\n",
    "This greatly simplifies the problem, since you don't need to work out the reward that will be given at every state when calculating the total return obtained between a starting state and the end of the episode. Instead you only need to look ahead one step.\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy Evaluation\n",
    "\n",
    "Baby Robot's mum told him to never trust strangers, so he's a bit nervous about following our policy. What happens if we're lying to him and it's not the optimal policy?\n",
    "\n",
    "So, rather than using our policy, he instead decides to toss a coin and use that to decide which way to go. Every time he enters a new state he'll flip the coin. If it's heads he'll go forwards, tails he'll go backwards. Therefore each of these actions now has a 50% chance of being selected.\n",
    "\n",
    "What happens to the value of each state under this new policy and how do we go about calculating it?\n",
    "\n",
    "\n",
    "* Firstly, the value of a state will still be a measure of how good it is to be in that state. However, since Baby Robot is no longer heading straight for the exit, the total reward he's going to accumulate will decrease. In other words, because he's likely to be visiting more states, or the same states several times, and the reward given for moving from one state to the next is still -1, the total reward is going to become more negative.\n",
    "\n",
    "\n",
    "* Additionally, the value of a state still represents the expected return from that state. However, because Baby Robot is now following a random policy (referred to as a <b><i>Stochastic Policy</i></b> in RL), based on the toss of a coin, the number of steps from any state to the exit may vary. Therefore the value of a state now represents the average, or expected, reward that can be achieved when starting in that state.\n",
    "\n",
    "\n",
    "* Since a state's value is the expected reward obtainable from that state, its value may be used to help calculate the value of the previous state. This avoids needing to know all the rewards that will be accumulated during the episode. \n",
    "\n",
    "\n",
    "* Consequently, the value of any action is simply the reward given for taking this action plus the value of the next state where you end up. The contribution of this action towards the current state's value is then obtained by multiplying the value of the action by the probability of taking this action.\n",
    "\n",
    "\n",
    "\n",
    "This can all be summarised by the following equation:\n",
    "\n",
    "\n",
    "![](GoodImages/equation_5.png)\n",
    "\n",
    "<i><center>Equation 5: The value of state 's' under a stochastic policy 'π'.</center></i>\n",
    "\n",
    "\n",
    "As with equation 4, for a deterministic policy, the value of any action is given by the reward obtained for taking that action, plus the value of the next state where that action leads to. However, since under a stochastic policy there can be more than one action, the action's reward is multiplied by the probability of taking the action: <i>π(a|s) represents the probability of taking action 'a' from state 's' under policy 'π'</i>. \n",
    "\n",
    "These values are then summed, over all the actions for the state, which gives the <b><i>expected</i></b> reward value for state 's'. Effectively, the combination of the sum and the probability of taking an action gives the average value of the rewards returned from the actions.\n",
    "\n",
    "Under Baby Robot's new policy each of the 2 actions (forwards or backwards) are taken with a probability of 0.5 and the reward for taking any action is still -1. Therefore the value of any state will be:\n",
    "\n",
    "\n",
    "![](GoodImages/equation_6.png)\n",
    "\n",
    "<i><center>Equation 6: The value of state 's' with 2, equally probable, actions that both return a reward of -1.</center></i>\n",
    "\n",
    "Where sᶠ is the state moved to when choosing the forwards action and sᵇ is the next state when taking the backwards action.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Iterative Policy Evaluation\n",
    "\n",
    "For this simple level, calculating the optimal policy was easy. We simply started at the exit and worked our way backwards, adding another -1 reward every time we moved to the next state. But how do we go about calculating the state value when the chance of moving to the next state is random? \n",
    "(In the language of RL the chance of moving to the next state is referred to as <b><i>State Transition Probability</b></i>).\n",
    "\n",
    "We can do this by taking a similar approach to the calculation of the optimal policy values, except now, rather than being able to find the value of each state in a single sweep through all of the states, we'll need to do multiple sweeps. Each of these will give us a slightly better estimate of a state's true value.\n",
    "\n",
    "Initially, since we don't know the value of any of the states, let's assume that none of them return a reward, so we'll set all initial values to zero. By definition the reward of the exit, the terminal state, is also zero since this is where the episode ends."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4e8c3071cfd486aa16d90739c13fc8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=424)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=True,fill_center=True)\n",
    "robot_position = RobotPosition(level)\n",
    "policy_evaluation = PolicyEvaluation( level )\n",
    "\n",
    "level.show_values(policy_evaluation.end_values)\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Iterative Policy Evaluation: initial state values.</center></i>\n",
    "\n",
    "To start the iterative process we can begin in any state but, for simplicity, let's begin at Baby Robot's current location, the entrance to the level.\n",
    "\n",
    "Baby Robot tosses his coin. Heads he goes north, tails he goes south. So the probability of each action is 0.5, the reward for either action is -1 and value of the next state is currently 0 for both actions. Therefore, using equation 6 above, the current value of the current state is:\n",
    "\n",
    "![](GoodImages/iterative_policy_evaluation_value.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To keep things simple we'll take the initial value of each state at the beginning of the sweep, rather than its updated value, to avoid the condition where some states have been updated and some haven't (although doing this is a perfectly reasonable thing to do and can often lead to a faster convergence of the state values; using the updated values is referred to as 'in-place' updating). \n",
    "\n",
    "Therefore, keeping the value of each next state at zero for this sweep, we can repeat the above procedure for the remaining states. This results in a value of -1 for all states at the end of the first pass.\n",
    "\n",
    "So, at the end of the first pass, the value of each state looks like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "324fffe242734f45ba05b033873757dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=324)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=False,fill_center=True)\n",
    "robot_position = RobotPosition(level)\n",
    "\n",
    "policy_evaluation.do_iteration()\n",
    "level.show_values(policy_evaluation.end_values)\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Iterative Policy Evaluation: state values at the end of the first sweep.</center></i>\n",
    "\n",
    "Once the value of each state has been calculated, the process can be repeated, using the newly calculated state values to calculate the state values of the next iteration. The progress in calculating the state values, over the first 10 iterations, is shown below (ignore the blue arrows for now, we'll come to those shortly)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "973e9bf8d30f445088876c2bee43ed08",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=324), HBox(children=(Play(value=1, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],fill_center=True,add_compass=False)\n",
    "robot_position = RobotPosition(level)\n",
    "policy_evaluation = PolicyEvaluation( level )\n",
    "\n",
    "directions = policy.get_directions(policy_evaluation.end_values)\n",
    "level.show_directions(directions)\n",
    "level.show_values(policy_evaluation.end_values)\n",
    "level.side_panel_text(158,104,f\"Iteration: {policy_evaluation.get_iterations()}\")\n",
    "\n",
    "def on_update(*args):        \n",
    "  policy_evaluation.do_iteration()\n",
    "  directions = policy.get_directions(policy_evaluation.end_values)\n",
    "  level.show_directions(directions)\n",
    "  level.show_values(policy_evaluation.end_values)    \n",
    "  level.side_panel_text(158,104,f\"Iteration: {policy_evaluation.get_iterations()}\")\n",
    "  \n",
    "play, progress, layout = setup_play_level( level, on_update, max=10 )\n",
    "VBox((level.canvases, HBox((play, progress))),layout=layout)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Iterative Policy Evaluation: iterations 0 to 9.</center></i>\n",
    "\n",
    "If this process is repeated for long enough the state values eventually stop increasing and are said to have reached convergence. In theory convergence is truly only reached \"in the limit\" or, in other words, when the number of time-steps is equal to infinity. Obviously this is rather impractical and so we instead define convergence to have occurred when the maximum difference between a state value at one iteration and the next is less than some threshold value. In our experiments we use a threshold of 1e-3 (=0.001) and, with this, it takes 206 iterations for the state values to converge for this policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence in 206 iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73c3467cc0e14c7ba122a5807391a3e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=324)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=False,fill_center=True)\n",
    "robot_position = RobotPosition(level)\n",
    "policy_evaluation = PolicyEvaluation( level )\n",
    "\n",
    "iterations = policy_evaluation.run_to_convergence(max_iterations = 300)\n",
    "directions = policy.get_directions(policy_evaluation.end_values)\n",
    "level.show_directions(directions)\n",
    "level.show_values(policy_evaluation.end_values)\n",
    "\n",
    "print(f\"Convergence in {iterations} iterations\")\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Iterative Policy Evaluation: the converged state values, reached after 206 iterations.</i></center>\n",
    "\n",
    "\n",
    "\n",
    "Now it can be seen that, under this policy of choosing the next state by tossing a coin, the values of each state are a lot more negative than under the optimal policy of going straight to the exit. The values do however still represent the expected number of steps from any state to the exit, except now Baby Robot is following a random trajectory that will lead to many more states being visited. This is shown below, for one of his shorter trips from the start to the exit of the level:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "730251d9490e4fc0aa6e366fbb731b5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=324), HBox(children=(Play(value=1, max=400,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],fill_center=True,add_compass=False)\n",
    "robot_position = RobotPosition(level)\n",
    "policy = Policy(level)  \n",
    "\n",
    "last_direction = None\n",
    "grid_step = 0\n",
    "level.side_panel_text(158,104,f\"Step: {grid_step}\")\n",
    "\n",
    "\n",
    "def on_update(*args):  \n",
    "  global last_direction, grid_step \n",
    "  \n",
    "  # keep baby robot moving in the last direction until he reaches\n",
    "  # the next grid square\n",
    "  if ((robot_position.x%robot_position.robot_size == level.padding) \n",
    "  and (robot_position.y%robot_position.robot_size == level.padding)):\n",
    "      \n",
    "    x,y = robot_position.get_cell_position()\n",
    "    allowed_directions = policy.get_allowed_actions(x,y)\n",
    "    if len(allowed_directions) > 0:\n",
    "      last_direction = random.choice(allowed_directions)   \n",
    "      grid_step += 1\n",
    "      level.side_panel_text(158,104,f\"Step: {grid_step}\") \n",
    "    else:\n",
    "      last_direction = None     \n",
    "  \n",
    "  if last_direction is not None:\n",
    "    robot_position.partial_move( last_direction ) \n",
    "    \n",
    "play, progress, layout = setup_play_level( level, on_update, interval=100, max=400 )\n",
    "VBox((level.canvases, HBox((play, progress))),layout=layout)    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>A sample trajectory under a stochastic policy.</center></i>\n",
    "\n",
    "---\n",
    "\n",
    "# Policy Improvement\n",
    "\n",
    "\n",
    "Unsurprisingly, deciding which move to make based on the toss of a coin isn't a very good strategy. As shown above, under this policy it takes much longer to reach the exit. Additionally, the value of each state is much lower than under the optimal policy. However, although the state values are much worse, in terms of the reward that can be obtained, they do still give one important bit of information: the relative goodness of each state.\n",
    "\n",
    "Looking back at the final, converged, state values, it can be seen that the expected reward that can be obtained from the starting square is -32. So, on average it will take 32 moves to reach the exit from this point. Similarly, for the square immediately to the North of the start position, the expected reward is -35 and to the South it's -27. Therefore its easy to see that, to get to the exit in the shortest number of steps, its better to head South from the starting square.\n",
    "\n",
    "By repeating this one-step look ahead, and acting greedily with respect to the value of the next state, we can modify the stochastic coin toss policy to create a policy that moves in the direction of the greatest reward. In this manner we can improve the policy, to produce one that gives increased rewards. \n",
    "\n",
    "Indeed, after a single iteration of policy evaluation on this level, acting greedily with respect to the state values gives us the optimal policy. This is shown by the blue arrows, which can be seen to point in the direction of greatest reward and lead directly from the entrance to the exit of the level.\n",
    "\n",
    "One other interesting observation, when acting greedily with respect to the calculated state values, is that it may not be necessary to wait for the values to converge before the policy can be improved. Look again at the first few iterations for this level (conveniently copied here to avoid you having to scroll!):\n",
    "\n",
    "\n",
    "\n",
    "![](GoodImages/IterativePolicyEvaluation_0to9.gif)\n",
    "\n",
    "<i><center>Iterative Policy Evaluation: iterations 0 to 9.</center></i>\n",
    "\n",
    "\n",
    "Although, during policy evaluation, it takes 206 iterations for the state values to fully converge, it can be seen that by the 5th iteration greedy selection has already found the optimal policy. Indeed, for the start square, which is all we're really interested in, the optimal policy has been found by the 4th iteration. All future iterations are therefore redundant in terms of improving the policy. We'll make use of this observation as we look at more efficient ways of finding the best policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Policy Iteration\n",
    "\n",
    "\n",
    "With this very simple initial level we've just seen that by evaluating a stochastic policy, and then acting greedily with respect to the calculated state values, we were able to find the optimal policy in a single iteration, where an iteration consists of policy evaluation followed by policy improvement.\n",
    "\n",
    "Unfortunately, for more complex problems, this may not always be the case and, more often than not, multiple iterations will be required. Each of these will improve the policy, in terms of its value function, until eventually improvement ceases, at which point the optimal policy will have been located.\n",
    "\n",
    "We can illustrate this by, instead of starting with a stochastic policy where actions are chosen with a given probability, using a deterministic policy where a specific action is defined for each state. Additionally, to make things harder, we'll start with a policy that is deliberately poor, so we can observe the improvement in the policy at each step.\n",
    "\n",
    "Here's the policy we're going to use:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9e3d6100764a48118e8ace135487277d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=424)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=True,fill_center=True)\n",
    "\n",
    "# start with deterministic policy\n",
    "directions = np.array([[ 4, 8, 2, 8, 4],\n",
    "                       [ 1, 0, 0, 0, 4],\n",
    "                       [ 1, 8, 8, 0, 1]])\n",
    "\n",
    "level.show_directions(directions)\n",
    "level.show_cell_direction_text(directions)\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Policy Iteration: initial policy.</center></i>\n",
    "\n",
    "## Discounted Rewards\n",
    "\n",
    "One important point to note about this initial policy is that none of the actions ever lead to the exit. As a result the episode will never terminate. Obviously this will cause problems when evaluating the policy. At each iteration of policy evaluation the state values will continue to increase until, eventually, our computer will explode. The values will never converge.\n",
    "\n",
    "Luckily there's a way we can work around this problem. Currently our state value represents the total reward that can be obtained from that state. As we saw way back in equation 2, this is calculated as the sum of all the rewards that will be obtained, starting in the state and then following the policy thereafter. Since our initial policy never reaches the terminal state, this sum will just continue to grow. \n",
    "\n",
    "To prevent this from happening we introduce the idea of <b><i>discounted rewards</i></b>. Now, rather than the return simply being the sum of all the rewards that are accumulated from a state until the end of an episode, we progressively reduce the contribution of rewards. The further a reward is into the future, the less weight it will be given when calculating the state's return value.\n",
    "\n",
    "The formula for calculating the return now becomes:\n",
    "\n",
    "![](GoodImages/equation_7.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<i><center>Equation 7: discounted return</center></i>\n",
    "\n",
    "In this new discounted formula for the return value, '<i>γ</i>' (gamma) is the discount factor, where 0 ≤ <i>γ</i> ≤ 1. So the reward from each time step is multiplied by an increasing power of '<i>γ</i>'. When the value of the discount factor is less than one, this will act to progressively reduce the value of rewards from future time steps, until eventually their contribution to the overall return is effectively zero.\n",
    "\n",
    "For example, a value of 0.9 is commonly used as the discount factor. With this we can calculate the return value of the initial state, as follows:\n",
    "\n",
    "\n",
    "\n",
    "![](GoodImages/initial_state_return.png)\n",
    "\n",
    "\n",
    "\n",
    "Clearly, applying a discount factor decreases the future return values and it doesn't take long before they're down close to zero.\n",
    "\n",
    "However, as we've seen, it would be impractical to calculate a state's value by considering the reward from all future states. Instead we use dynamic programming to simplify the problem into one that just uses the immediate reward and value of the next state. The value of the next state represents the return that will be obtained from the next state and therefore we can just change equation 7 to apply the discount factor to the next state's value:\n",
    "\n",
    "\n",
    "![](GoodImages/equation_8.png)\n",
    "\n",
    "<i><center>Equation 8: The value of state 's' under a stochastic policy 'π' with discounted future rewards.</center></i>\n",
    "\n",
    "\n",
    "\n",
    "In other words: <i>the value of a state, when following policy 'π', is equal to the sum, over all actions from that state, of the probability of taking each action, times the immediate reward for that action plus the discounted value of the next state where we end up after taking the action.</i>\n",
    "\n",
    "Using the discounted state value function, with the discount factor set to 0.9, we can now calculate the value of our deterministic policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6896e0295b74d9b954b6ace6258da07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=324)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=False,fill_center=True)\n",
    "\n",
    "policy_evaluation = PolicyEvaluation( level )\n",
    "policy_evaluation.set_policy(directions)\n",
    "policy_evaluation.set_discount_factor(0.9)\n",
    "\n",
    "iterations = policy_evaluation.run_to_convergence(max_iterations = 300)\n",
    "level.show_values(policy_evaluation.end_values)\n",
    "level.side_panel_text(158,104,f\"Iteration: {policy_evaluation.get_iterations()}\")\n",
    "\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Policy Iteration: discounted state values.</center></i>\n",
    "\n",
    "With discounted rewards, the value of each state, rather than continually decreasing, now converges to our threshold in 67 iterations.\n",
    "\n",
    "One important point to note about the new, discounted, state values is the following:\n",
    "\n",
    "* The state values no longer represent the expected number of steps to the exit. Instead they now show the expected discounted future reward from each state under this policy.\n",
    "\n",
    "Looking at the state values that have been calculated it can be seen that they are all equally bad. All that is apart from one: the exit, which as usual has a value of zero. If we now update our policy by acting greedily with respect to these values we get the following policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8ca4d2410f5043ed8db9df627d55b689",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=424)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=True,fill_center=True)\n",
    "\n",
    "policy = Policy(level) \n",
    "policy.set_policy(directions)\n",
    "policy.update_policy(policy_evaluation.end_values)\n",
    "\n",
    "new_policy = policy.get_policy()\n",
    "level.show_directions(new_policy)\n",
    "level.show_cell_direction_text(new_policy)\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Policy Iteration: selecting greedily with respect to the state values.</center></i>\n",
    "\n",
    "Notice that the policy has now changed so that, when in either of the squares adjacent to the exit, the defined action is now to move to the exit. Additionally, because after the first round of policy evaluation all states other than the exit had values of -10 we've chosen to break the tie, when selecting greedily, by retaining the action from the original policy. Another approach to tie breaking is to just choose arbitrarily between the tied actions.\n",
    "\n",
    "Repeating this process of running pairs of policy evaluation followed by policy improvement results in the policy updating as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6084b9cd448143e4b047f33770b5e183",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=324), HBox(children=(Play(value=1, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=False,fill_center=True)\n",
    "\n",
    "policy_evaluation = PolicyEvaluation( level )\n",
    "policy_evaluation.set_policy(directions)\n",
    "policy_evaluation.set_discount_factor(0.9)\n",
    "\n",
    "policy = Policy(level) \n",
    "policy.set_policy(directions)\n",
    "new_policy = policy.get_policy()\n",
    "\n",
    "step = 0\n",
    "level.show_directions(directions)\n",
    "level.show_cell_direction_text(directions)\n",
    "level.side_panel_text(158,104,f\"Iteration: {step}\")\n",
    "\n",
    "def on_update(*args):   \n",
    "  global new_policy, step\n",
    "  \n",
    "  # note that policy evaluation starts with the state values from the previous policy\n",
    "  # - this greatly reduces the number of iterations required to run policy evaluation to convergence \n",
    "  policy_evaluation.set_policy(new_policy)\n",
    "  policy_iterations = policy_evaluation.run_to_convergence(max_iterations = 300)  \n",
    "  policy.update_policy(policy_evaluation.end_values)\n",
    "  new_policy = policy.get_policy()\n",
    "  level.show_directions(new_policy)\n",
    "  level.show_cell_direction_text(new_policy) \n",
    "  step += 1\n",
    "  level.side_panel_text(158,104,f\"Iteration: {step} Policy: {policy_iterations}\")\n",
    "  \n",
    "play, progress, layout = setup_play_level( level, on_update, max=8 )\n",
    "VBox((level.canvases, HBox((play, progress))),layout=layout)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Policy Iteration: run until policy has stabilised.</center></i>\n",
    "\n",
    "\n",
    "In the diagram above, \"<i>Iteration</i>\" shows the current policy iteration step and \"<i>Policy</i>\" shows the number of iterations needed to run policy evaluation to convergence. Note how the number of policy evaluation steps reduces substantially after the first policy iteration step. This is due to retaining the previous policy's converged state values when starting the next policy evaluation. Since the new policy is based on the previous policy, it will have similar state values. Therefore, using the previous policy's values greatly reduces the number of iterations required to run policy evaluation on the new policy.\n",
    "\n",
    "Policy Iteration takes an initial policy, evaluates it, and then uses those values to create an improved policy. These steps of evaluation and improvement are then repeated on the newly generated policy to give an even better policy. This process continues until, eventually, we end up with the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# Value Iteration\n",
    "\n",
    "\n",
    "As we've seen, Policy Iteration evaluates a policy and then uses these values to improve that policy. This process is repeated until eventually the optimal policy is reached. As a result, at each iteration prior to the optimal policy, a sub-optimal policy has to be fully evaluated. Consequently, there is potentially a lot of wasted effort when trying to find the optimal policy.\n",
    "\n",
    "When we looked at Policy Improvement on our simple grid level, we saw that it was possible to find the optimal policy after only a few iterations of Policy Evaluation. We didn't need to run the evaluation step to full convergence. The number of state value calculations could have been reduced dramatically and we would still have obtained the best policy.\n",
    "\n",
    "Many algorithms use this idea to go beyond simple Policy Iteration and instead improve their policy after a reduced number of policy evaluation steps. Value Iteration takes this idea to the extreme, effectively reducing the evaluation stage down to a single sweep of the states. Additionally, to improve things further, it combines the Policy Evaluation and Policy Improvement stages into a single update.\n",
    "\n",
    "This can be seen in equation 9 below:\n",
    "\n",
    "\n",
    "\n",
    "![](GoodImages/equation_9.png)\n",
    "\n",
    "<i><center>\n",
    "Equation 9: Value Iteration. The value of state 's' at iteration 'k+1' is the value of the action that gives the maximum expected reward.\n",
    "</center></i>    \n",
    "\n",
    "\n",
    "\n",
    "At iteration of 'k+1' of Value Iteration, the value of state 's' is given by the value of the action that returns the maximum expected reward. Remember that an expected value is effectively the mean value. Since, in our simple environment, we always get given the same reward 'r' of -1 for taking an action, this equation can be simplified to:\n",
    "\n",
    "\n",
    "\n",
    "![](GoodImages/equation_10.png)\n",
    "\n",
    "<i><center>\n",
    "Equation 10. Value Iteration. The value of state 's' at iteration 'k+1' for actions that return a constant reward.\n",
    "</center></i>\n",
    "\n",
    "\n",
    "In a state 's', we calculate the value of each action by taking its immediate reward (in our case -1) and adding the discounted reward of the next state s′, using that state's current value. We then choose the largest of these calculated values and use this as the value of state 's' at the next iteration 'k+1'.\n",
    "\n",
    "Note that the state value at the next iteration is based on the current estimate of the next state's value.  When one estimate is based on another, this is known as <b><i>bootstrapping</i></b>.\n",
    "\n",
    "So, where Policy Iteration would evaluate a policy and then act greedily with respect to those calculated values to get the next policy, Value Iteration effectively works without a policy. Only at the end, once the optimal value function has been found, will the optimal policy be created from the state values.\n",
    "\n",
    "Value Iteration applied to the simple grid level is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5493536251f146a285edccabfa89d9b7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=324), HBox(children=(Play(value=1, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level = GridLevel(level_width,level_height,start=[0,1],end=[3,2],add_compass=False,fill_center=True)\n",
    "value_iteration = ValueIteration( level, discount_factor=1.0 )\n",
    "\n",
    "step = 0\n",
    "level.show_values(value_iteration.values)\n",
    "level.side_panel_text(157,104,f\"Iteration: {step}\")\n",
    "\n",
    "# the convergence threshold\n",
    "# - stop iteratating when the maximum state change value falls below this\n",
    "threshold = 1e-3\n",
    "\n",
    "def on_update(*args):   \n",
    "  global step\n",
    "  \n",
    "  # do one sweep of the states and get the maximum state change value\n",
    "  delta = value_iteration.state_sweep()  \n",
    "     \n",
    "  # test if the max state change is less than the defined convergence threshold\n",
    "  if delta < threshold:\n",
    "    # use the state values to greedily choose the policy\n",
    "    policy = Policy(level)\n",
    "    directions = policy.get_directions(value_iteration.values)    \n",
    "    level.show_directions(directions) \n",
    "    \n",
    "  step += 1      \n",
    "  level.show_values(value_iteration.values)  \n",
    "  level.side_panel_text(157,104,f\"Iteration: {step} Delta: {delta:.3f}\")\n",
    "  \n",
    "play, progress, layout = setup_play_level( level, on_update, max=8 )\n",
    "VBox((level.canvases, HBox((play, progress))),layout=layout)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i><center>Value Iteration: run until state values have stabilised.</center></i>\n",
    "\n",
    "Here we've once again returned to using non-discounted rewards, which is equivalent to setting the discount factor to 1, so each of the state values once again represents the non-discounted return from each state.\n",
    "\n",
    "The '<i>Delta</i>' value, shown in the centre of the image, represents the maximum state-action error at a particular iteration. Notice how this value is always 1, as the state-value updates propagate out from the exit of the level, until eventually, when all state values have stabilised this drops to zero. At this point we can select greedily with respect to these values to get the policy. \n",
    "\n",
    "For this very simple grid level it takes only 7 iterations for the state values to converge and for the optimal policy to be discovered. When you compare this against the 206 iterations that are required to run policy evaluation to convergence, it can be seen how Value Iteration is a vast improvement over simple Policy Iteration. Additionally, since each iteration represents a sweep over all of the states, the reduction in the total number of iterations required to find the optimal policy will be of even greater benefit in more complex environments, featuring much larger state spaces.\n",
    "\n",
    "At the very beginning of this article Baby Robot can be seen unsuccessfully trying to find his way out of a large maze. Applying Value Iteration to this maze gives the state values, and policy obtained by acting greedily with respect to these values, shown below. If this greedy policy is followed then, for any square in this maze, the state value represents the exact number of steps to the exit. As can be seen, Baby Robot has followed this path and has now successfully navigated his way to the exit of the maze. \n",
    "\n",
    "To calculate all of these state values using Value Iteration took 30 iterations (you'll notice that this is exactly equal to the state value at the entrance of the maze, the furthest point away from the exit). In contrast, with discounted returns, Policy Evaluation took 66 iterations to converge.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Values converged in 30 iterations\n"
     ]
    }
   ],
   "source": [
    "level_height = 6\n",
    "level_width = 10\n",
    "\n",
    "level = GridLevel( level_width, \n",
    "                   level_height, \n",
    "                   add_maze = True,\n",
    "                   add_compass = False, \n",
    "                   fill_center = False, \n",
    "                   maze_seed = 3456,\n",
    "                   end = [9,3],\n",
    "                   show_start_text = True)\n",
    "\n",
    "robot_position = RobotPosition(level,initial_sprite = 2)\n",
    "\n",
    "value_iteration = ValueIteration( level, discount_factor = 1.0 )\n",
    "iterations = value_iteration.run_to_convergence()\n",
    "\n",
    "policy = Policy(level)\n",
    "directions = policy.get_directions(value_iteration.values)\n",
    "\n",
    "# now select greedily wrt the converged values\n",
    "policy = Policy(level)\n",
    "directions = policy.get_directions(value_iteration.values)\n",
    "level.show_directions(directions)\n",
    "level.show_values(value_iteration.values)\n",
    "\n",
    "print(f\"Values converged in {iterations} iterations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55ff5252644b41ab8a53a8ab86dff3a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=388, image_data=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02\\x84\\x00\\x0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def on_update(*args):    \n",
    "  x,y = robot_position.get_cell_position()   \n",
    "  end = level.get_end()\n",
    "  robot_position.move( Direction(directions[y,x]) )\n",
    "    \n",
    "play, progress, layout = setup_play_level( level, on_update, interval=250, max=200 )\n",
    "VBox((level.canvases, HBox((play, progress))),layout=layout) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Iteration applied to Large Maze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "level_height = 6\n",
    "level_width = 10\n",
    "\n",
    "level = GridLevel( level_width, \n",
    "                   level_height, \n",
    "                   add_maze = True,\n",
    "                   add_compass = False, \n",
    "                   fill_center = False, \n",
    "                   maze_seed = 3456,\n",
    "                   end = [9,3],\n",
    "                   show_start_text = True)\n",
    "\n",
    "# initially make all directions equally likely\n",
    "directions = np.full(\n",
    "    shape=(level_height, level_width),\n",
    "    fill_value=Direction.All,\n",
    "    dtype=np.int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3081b9749da944149ce2c43d247b2940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=388, image_data=b'\\x89PNG\\r\\n\\x1a\\n\\x00\\x00\\x00\\rIHDR\\x00\\x00\\x02\\x84\\x00\\x00\\x01\\x84\\x08\\x…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the initial maze and policy\n",
    "level.show_directions(directions)\n",
    "level.show_cell_direction_text(directions)\n",
    "level.canvases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iterations to policy evaluation convergence = 66\n"
     ]
    }
   ],
   "source": [
    "# run policy evaluation on the maze\n",
    "policy_evaluation = PolicyEvaluation( level )\n",
    "policy_evaluation.set_policy(directions)\n",
    "policy_evaluation.set_discount_factor(0.9)\n",
    "\n",
    "iterations = policy_evaluation.run_to_convergence(max_iterations = 300)\n",
    "level.show_values(policy_evaluation.end_values)\n",
    "print(f\"Iterations to policy evaluation convergence = {iterations}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show the final policy\n",
    "policy = Policy(level) \n",
    "new_policy = policy.get_directions(policy_evaluation.end_values)\n",
    "level.show_directions(new_policy)\n",
    "level.show_cell_direction_text(new_policy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9422e663f91488b8a01300085461f3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=388, sync_image_data=True, width=644), HBox(children=(Play(value=1, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "level_height = 6\n",
    "level_width = 10\n",
    "\n",
    "level = GridLevel( level_width, \n",
    "                   level_height, \n",
    "                   add_maze = True,\n",
    "                   add_compass = False, \n",
    "                   fill_center = False, \n",
    "                   maze_seed = 3456,\n",
    "                   end = [9,3],\n",
    "                   show_start_text = True)\n",
    "\n",
    "robot_position = RobotPosition(level)\n",
    "policy_evaluation = PolicyEvaluation( level )\n",
    "\n",
    "directions = policy.get_directions(policy_evaluation.end_values)\n",
    "level.show_directions(directions)\n",
    "level.show_values(policy_evaluation.end_values)\n",
    "\n",
    "def on_update(*args):        \n",
    "  policy_evaluation.do_iteration()\n",
    "  directions = policy.get_directions(policy_evaluation.end_values)\n",
    "  level.show_directions(directions)\n",
    "  level.show_values(policy_evaluation.end_values)    \n",
    "  \n",
    "play, progress, layout = setup_play_level( level, on_update, max=50)\n",
    "VBox((level.canvases, HBox((play, progress))),layout=layout)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary\n",
    "\n",
    "After all the running through mazes Baby Robot is pretty tired (as I'm sure you are too!), so we'll take a break here. We've managed to successfully get Baby Robot through the very simple initial level and helped him to navigate a much more complicated maze. In the process we covered nearly all of the main foundations of Reinforcement Learning:\n",
    "\n",
    "\n",
    "* Reinforcement Learning uses the concept of rewards to drive learning. For the states that make up the problem environment, a value can be calculated from the rewards to represent how good it is to be in each state. Then, by choosing the actions that maximise the rewards, it's possible to find the optimal policy that can be applied to solve the problem at hand. \n",
    "\n",
    "\n",
    "* The value for each state represents the return that can be obtained if you start in that state and then follow the current policy until the end of the episode. Discounted rewards make it possible to focus the state values on rewards that would be obtained in the close future. Additionally they can prevent problems that may otherwise occur if the episode is not guaranteed to terminate. \n",
    "\n",
    "\n",
    "* The calculation of state values can be greatly simplified by using Dynamic Programming. Rather than having to know all future rewards from a particular state, the problem is split into the immediate reward and the value of the next state. \n",
    "\n",
    "\n",
    "* Using the dynamic programming, we were able to perform Policy Evaluation, in which we calculated the value of all states under the current policy.\n",
    "\n",
    "\n",
    "* Once Policy Evaluation was complete the state values could then be used to improve the policy by selecting greedily with respect to these values. By repeatedly running stages of evaluation and improvement Policy Iteration could be used to find the optimal policy.\n",
    "\n",
    "\n",
    "* Value Iteration improved on the efficiency of Policy Iteration by setting the state value to the maximum action value for that state. Once these state values had converged, to the optimal state values, the optimal policy could then be obtained.\n",
    "\n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# What's Next?\n",
    "\n",
    "Although we've covered a lot of ground, we're still missing a few of the core concepts of Reinforcement Learning. In particular, <b><i>Markov Decision Processes</i></b> and <b><i>Bellman Equations</i></b>. In simple terms these are, respectively, the mathematical framework used to model Reinforcement Learning problems and the set of equations used to calculate the values of states and actions. As you've seen, we've already used some equations to calculate the state and action values. These are actually partial forms of the full Bellman Equations. We'll fully describe both of these topics in the next post.\n",
    "\n",
    "Additionally, Baby Robot has not yet had to do any exploration of the levels where he's found himself. All of the information about the level, such as the rewards and state transition probabilities were already given and we used these to derive the optimal policy for each level. When all the information is given up front it's known as a <b><i>model-based</i></b> system. A more realistic scenario is when these values aren't available and some exploration is required to solve the problem. Unsurprisingly, problems of this nature are called <b><i>model-free</i></b> problems and we'll take a look into these in future posts.\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "![](GoodImages/green_babyrobot_small.gif)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Footnotes:\n",
    "\n",
    "<b><i>Top-Down Learning</i></b>: This approach to learning is probably most famously used (at least in the machine-learning world) by Jeremy Howard in his excellent [Fast.ai](https://www.fast.ai/2019/01/24/course-v3/) courses. In [academic studies](https://www.gse.harvard.edu/news/uk/09/01/education-bat-seven-principles-educators) it's been shown to help give a better understanding of the overall concepts involved in a subject."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References\n",
    "\n",
    "For a full theoretical breakdown of everything covered in this article, check out the bible of Reinforcement Learning:  \"[Reinforcement Learning: An Introduction](http://www.incompleteideas.net/book/RLbook2020.pdf)\", Sutton & Barto (2018)\n",
    "\n",
    "\n",
    "For the Baby Robot's Guide to Multi-Armed Bandits, start here:\n",
    "[Multi-Armed Bandits: Part 1 - \n",
    "Mathematical Framework and Terminology](https://towardsdatascience.com/multi-armed-bandits-part-1-b8d33ab80697)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
