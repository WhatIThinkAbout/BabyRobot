{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[A Baby Robot's Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)__\n",
    "\n",
    "# State Values and Policy Evaluation in 5 minutes\n",
    "\n",
    "## An Introduction to Reinforcement Learning\n",
    "\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/State_Values_and_Policy_Evaluation_Cover_Opt.gif\"/></center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/WhatIThinkAbout/BabyRobot/HEAD?labpath=Reinforcement_Learning%2FPart%201%20Summary%20-%20State%20Values%20and%20Policy%20Evaluation%20in%205%20Minutes.ipynb)\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "This is a summary of the article <b>[State Values and Policy Evaluation](https://medium.com/towards-data-science/state-values-and-policy-evaluation-ceefdd8c2369)</b>. It distils all of the key terms and theory from that article down into a single cheat-sheet that can be read in 5 minutes or less. With that in mind, we'd better get started…\n",
    "\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<center><img src=\"images/green_babyrobot_small.gif\"/></center>\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "\n",
    "## Reinforcement Learning\n",
    "Reinforcement Learning can be considered to be a problem that takes place in an environment that consists of multiple, independent, states.\n",
    "A simple example of this would be a grid world, where each square in the grid represents a state:\n",
    "\n",
    "<br/>\n",
    "<br/>\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/basic_grid.png\"/></center>\n",
    "\n",
    "### State\n",
    "A unique, self-contained, stage in the environment that defines the current situation. Each state is independent of previous states, which means you don't need to know or remember what has happened before.\n",
    "\n",
    "### Episode\n",
    "One complete execution of the environment.\n",
    "\n",
    "### Terminal State\n",
    "The final state in an environment where the episode ends.\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "\n",
    "### Agent\n",
    "The agent is the thing that interacts with the environment and that makes decisions on how to move through the environment. In our case Baby Robot is the agent.\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/agent.png\"/></center>\n",
    "\n",
    "To move from one state **_s_** to the next state **_s'_** (read as s-prime) the agent takes an action a. As a result of taking this action it receives a reward **_r_**.\n",
    "\n",
    "### Reward\n",
    "The numerical value used to measure performance. It can be expressed as a penalty by making it negative. The more negative the worse the performance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Expected reward for a state-action pair\n",
    "The expected reward can be thought of as the average reward and is used if the reward given for a particular action can vary.\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/expected_reward.png\" style=\"background-color:#FFFFFF\"/></center>\n",
    "\n",
    "\n",
    "At time '_t-1_', starting in a particular state **_s_** and taking action **_a_**, the expected reward that's received at the next time step is a function of the current state and action."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Return 'Gₜ'\n",
    "The total amount of reward accumulated over an episode, starting at time 't' is equal to the sum of future rewards:\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/return.png\" style=\"background-color:#FFFFFF\"/></center>\n",
    "\n",
    "***"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy\n",
    "The strategy that is used by the agent to choose an action in a state.\n",
    "In equations the policy is typically represented by '_π_'.\n",
    "\n",
    "### Greedy Policy\n",
    "Selects the action with the highest immediate reward."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Reinforcement Learning can be split into two distinct parts, the Prediction Problem and the Control Problem.\n",
    "\n",
    "### Prediction Problem\n",
    "Evaluate the performance of an agent.\n",
    "\n",
    "### State Value\n",
    "A measure of how good it is to be in that state. Given by the expected reward that can be obtained from starting in that state and then following the current policy for all future states.\n",
    "\n",
    "The value for state s under policy **_π_** is the expected return:\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/state_value.png\" style=\"background-color:#FFFFFF\"/></center>\n",
    "\n",
    "\n",
    "For a **_deterministic policy_**, where a single action is always selected in each state and when you're guaranteed of getting the same reward and ending up in the same next state, the value of a state is simply the immediate reward plus the value of the next state:\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/deterministic_policy.png\" style=\"background-color:#FFFFFF\"/></center>\n",
    "\n",
    "\n",
    "For a **_stochastic policy_**, where multiple actions are possible, π(a|s) represents the probability of taking action a from state s under policy π.\n",
    "In this case the state value is given by the sum of each action's reward multiplied by the probability of taking that action:\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/stochastic_policy.png\" style=\"background-color:#FFFFFF\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Dynamic Programming\n",
    "Splits the problem into simpler sub-problems. In this case the state value is calculated by splitting the problem into 2 parts: the immediate reward and the value of the next state.\n",
    "\n",
    "State values are calculated using the values of other states. As a result you only need to look one step ahead and don't need to know all of the rewards accumulated during the episode.\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## Iterative Policy Evaluation\n",
    "By repeatedly applying the above equation for the state value the converged state value can be calculated.\n",
    "\n",
    "The algorithm for this is:\n",
    "* Start with all state values set to zero.\n",
    "* Do a sweep of all states to calculate the state values (using the above equation).\n",
    "* Repeat until convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is shown in the code below, using the BabyRobot custom Gym environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "# get the latest version of the babyrobot custom gym environment\n",
    "%pip install babyrobot --upgrade -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2a49dbb23f347d9be94bad68f9941a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import babyrobot\n",
    "from babyrobot.lib import Policy\n",
    "from babyrobot.lib import PolicyEvaluation\n",
    "\n",
    "# create a blank, default environment\n",
    "setup = {'show_start_text':False,'show_end_text':False,'robot':{ 'show': False}}\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "\n",
    "# create a stochastic policy and a policy evaluation object for this policy\n",
    "policy = Policy(env)\n",
    "policy_evaluation = PolicyEvaluation( env, policy )\n",
    "\n",
    "# show the initial state values\n",
    "info = {'text': policy_evaluation.end_values}\n",
    "env.show_info(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence after 105 iterations\n"
     ]
    }
   ],
   "source": [
    "# iterate until convergence\n",
    "iterations = 0\n",
    "convergence = False\n",
    "while convergence == False:\n",
    "  info = {'text': policy_evaluation.end_values}\n",
    "  env.show_info(info)\n",
    "  policy_evaluation.do_iteration()\n",
    "\n",
    "  # calculate the largest difference in the state values from the start to end of the iteration\n",
    "  delta = np.max(np.abs(policy_evaluation.end_values - policy_evaluation.start_values))\n",
    "\n",
    "  # test if the difference is less than the defined convergence threshold\n",
    "  if delta < policy_evaluation.threshold:\n",
    "    convergence = True  \n",
    "\n",
    "  iterations += 1\n",
    "\n",
    "print(f\"Convergence after {iterations} iterations\")  "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And here's one I made earlier, showing the state values after every 5 iterations of Policy Evaluation and rounded to 1 decimal place:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/Iterative_Policy_Evaluation.gif\" style=\"background-color:#FFFFFF\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was created using the BabyRobot Animate class, as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5956237c0d20413da32386b4e0d749b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=346), HBox(children=(Play(value=0, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from babyrobot.lib import Animate\n",
    "\n",
    "setup = {'show_start_text':False,'show_end_text':False,'robot':{ 'show': False}}\n",
    "setup['side_panel'] = {'width':150}\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "\n",
    "# create a stochastic policy and a policy evaluation object for this policy\n",
    "policy = Policy(env)\n",
    "policy_evaluation = PolicyEvaluation( env, policy )\n",
    "\n",
    "animate = Animate(env)\n",
    "args = { 'max_steps': 105, 'save_interval': 5, 'show_directions': False }\n",
    "animate.show_policy_evaluation(policy_evaluation,**args)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Control Problem\n",
    "Modify the policy to improve performance.\n",
    "\n",
    "## Policy Improvement\n",
    "After the state values have been calculated act greedily with respect to these values to form a new policy (i.e. choose the action which will take you to the next state that has the greatest state value).\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/Policy_Improvement.png\" style=\"background-color:#FFFFFF\"/></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence in 104 iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "378ffc3cbb6a44a09efa0d97a9af0bc9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a blank, default environment\n",
    "setup = {'show_start_text':False,'show_end_text':False,'robot':{ 'show': False}}\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "\n",
    "# create a stochastic policy and a policy evaluation object for this policy\n",
    "policy = Policy(env)\n",
    "policy_evaluation = PolicyEvaluation( env, policy )\n",
    "\n",
    "# run policy evaluation to convergence\n",
    "steps_to_convergence = policy_evaluation.run_to_convergence(max_iterations = 300)\n",
    "print(f\"Convergence in {steps_to_convergence} iterations\")\n",
    "\n",
    "# show the final state values after convergence\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "directions = policy.get_directions(values=policy_evaluation.end_values)\n",
    "info = {'text': policy_evaluation.end_values, 'precision': 1,\n",
    "        'directions': {'arrows':directions}}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Discounted Rewards\n",
    "Progressively reduce the contribution of rewards from future time steps by using a discount factor **_γ_** (gamma), where 0 ≤ γ ≤ 1.\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/Discounted_Rewards.png\" style=\"background-color:#FFFFFF\"/></center>\n",
    "\n",
    "This allows the calculation of state values for deterministic policies that may not terminate. A value of 0.9 is commonly used as the discount factor.\n",
    "\n",
    "The value of state **_s_** under policy <span style=\"font-family: arial;\">**_π_**</span> with discounted future rewards is given by:\n",
    "\n",
    "<center><img src=\"images/state_values_and_policy_evaluation_5mins/State_Value_Discounted_Rewards.png\" style=\"background-color:#FFFFFF\"/></center>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "We've now covered all of the basics of Reinforcement Learning (RL) and all in record time!\n",
    "\n",
    "We concluded our introduction with an equation that's a cut-down version of the **Bellman Equation**, the key equation in Reinforcement Learning. In _[Part 2](https://medium.com/towards-data-science/markov-decision-processes-and-bellman-equations-45234cce9d25)_ of this series we look at the full Bellman Equation and examine **Markov Decision Processes** which are another core concept of RL."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "<h4 style='color:green'>\n",
    "If you found this notebook interesting and/or useful please give the accompanying Medium article a like.</br> \n",
    "Thanks!\n",
    "</h4>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "BabyRobotGym",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "50f7925c2b527e04ad4ab9285d4738429ed4ef149c3803ef7aee3c43b8d710c9"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
