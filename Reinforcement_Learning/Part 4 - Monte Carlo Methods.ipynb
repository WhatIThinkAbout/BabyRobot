{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Once again we're off to the casino, and this time it's situated in sunny Monte Carlo, made famous by its appearance in the classic movie _[Madagascar 3: Europe's Most Wanted](https://en.wikipedia.org/wiki/Madagascar_3:_Europe%27s_Most_Wanted)_ (although there's a slight chance that it was already famous).\n",
    "\n",
    "In our last visit to a casino we looked at the _[multi-armed bandit](https://medium.com/towards-data-science/multi-armed-bandits-part-1-b8d33ab80697)_ and used this as a way to visualise the problem of how to choose the best action when confronted with many possible actions.\n",
    "\n",
    "In terms of **Reinforcement Learning** the bandit problem can be thought of as representing a single state and the actions available within that state. _Monte Carlo_ methods extend this idea to cover multiple, interrelated, states.\n",
    "\n",
    "Additionally, in the previous problems we've looked at, we've always been given a full model of the environment. This model defines both the transition probabilities, that describe the chances of moving from one state to the next, and the reward received for making this transition.\n",
    "\n",
    "In _Monte Carlo_ methods this isn't the case. No model is given and instead the agent must discover the properties of the environment through exploration, gathering information as it moves from one state to the next. In other words, <i>Monte Carlo methods learn from experience.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/green_babyrobot_small.gif\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This notebook accompanies the Towards Data Science article and is part of _[A Baby Robot's Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)_\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Setup\n",
    "\n",
    "The examples in this notebook use the **[Baby Robot Custom Gym Environment](https://medium.com/towards-data-science/creating-a-custom-gym-environment-for-jupyter-notebooks-e17024474617)**.\n",
    "\n",
    "The source code for this can be found on _[Github](https://github.com/WhatIThinkAbout/BabyRobotGym)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby Robot Version = 1.0.31\n"
     ]
    }
   ],
   "source": [
    "# install Baby Robot Gym\n",
    "# %pip install --upgrade babyrobot -q\n",
    "\n",
    "import babyrobot\n",
    "print(f\"Baby Robot Version = {babyrobot.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Prediction\n",
    "\n",
    "In the prediction problem we want to find how good it is to be in a particular state of the environment. This \"_goodness_\" is represented by the state value, which is defined as the expected reward that can be obtained when starting in that state and then following the current policy for all subsequent states.\n",
    "\n",
    "When we have full knowledge about the environment, and know the transition probabilities and rewards, we can simply use _[Dynamic Programming](https://medium.com/towards-data-science/state-values-and-policy-evaluation-ceefdd8c2369#e996)_ to iteratively calculate the value for each state.\n",
    "\n",
    "In practice, its unlikely that a system's transition probabilities are known in advance. Therefore, to estimate how likely it is to move from one state to another, it's possible to observe multiple episodes and then take the average. This approach, of taking random samples to calculate estimates, is known as **_Monte Carlo Sampling_**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the level, shown in figure 1 below, where Baby Robot currently finds himself:\n",
    "\n",
    "<center><img src=\"images/part4/glass_wall_level.png\"/></center>\n",
    "<center><i>Figure 1: A level containing a glass wall and the co-ordinates on thisÂ level.</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance this level appears to be rather simple, with a short path from the start of the level to the exit. However, there are 2 obstacles of note:\n",
    "\n",
    "* In the top-middle square (coordinate (1,0)) there is a large puddle. As we've seen before, Baby Robot doesn't like puddles. They take longer to move through, incurring a negative reward of -4, and can cause him to skid.\\\n",
    "\\\n",
    "When a skid occurs Baby Robot won't reach the target state. Normally this would result in him moving to one of the other possible states, but in this case there are no other possible states, so he'll stay exactly where he is and receive another -4 penalty.\\\n",
    "\\\n",
    "If Baby Robot moves into this puddle there's a good chance he'll become stuck for several time periods and receive a large negative reward. It would be best to avoid this puddle!\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "* The thick blue line, between the cells (1,1) and (1,2), represents a glass wall. This is a new type of challenge that Baby Robot hasn't encountered before.\\\n",
    "\\\n",
    "Unlike standard walls, Baby Robot can't see glass walls and may therefore select an action that causes him to walk into the wall. When this happens he'll bounce off the wall and, rather than reaching the target state, end up in the opposite state. Also, he'll be given a negative reward penalty of -1 for the additional time required to make the move.\\\n",
    "\\\n",
    "In this level there are 2 possible opportunities for walking into the glass wall:\n",
    "\n",
    "  - if he moves South from cell (1,1) he'll instead end up in the puddle at (1,0) and receive a reward of -5 (-4 for moving into a puddle and -1 for hitting the glass wall).\n",
    "\n",
    "  - if he moves North from (1,2), instead of arriving at (1,1) he'll actually bounce off the wall and end up at the exit. In this case he'll be given a reward of -2 (-1 wall penalty and -1 for moving to a dry square)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "basesetup = {'width':3,'height':4,'start':[0,1],'end': [1,3]}\n",
    "\n",
    "# add a glass wall\n",
    "basesetup['walls'] = [((1, 1),'S',{'color':'#00a9ff','width':10,'fit':True,'prob':0.0})]\n",
    "basesetup['puddles'] = [((1,0),2)]   \n",
    "basesetup['base_areas'] = [(0,0,1,1),(0,2,1,2),(2,0,1,1),(2,3,1,1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53da5ba921b54caeabf518a51c6892b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create the test environment using the base setup\n",
    "setup = deepcopy(basesetup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a setup for information display \n",
    "# - just show the graphical grid\n",
    "info_setup = deepcopy(basesetup)\n",
    "info_setup['show_start_text'] =False\n",
    "info_setup['show_end_text'] = False\n",
    "info_setup['robot'] = { 'show': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfab746c04b643dcb49e2c085d9a60ee",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=296)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "setup = deepcopy(info_setup)\n",
    "setup['add_compass'] = True\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "info = {'coords': True}\n",
    "env.show_info(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "As mentioned above, when we have complete information about a system, and know all of its transition probabilities and rewards, we can use _[Policy Evaluation](https://towardsdatascience.com/state-values-and-policy-evaluation-ceefdd8c2369)_ to calculate the state values.\n",
    "\n",
    "For this environment with a stochastic policy, that just chooses randomly between the available actions in each state, _Policy Evaluation_ gives the following state values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a stochastic policy and a policy evaluation object for this policy\n",
    "policy = Policy(env)\n",
    "policy_evaluation = PolicyEvaluation( env, policy )\n",
    "\n",
    "setup = deepcopy(info_setup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", render_mode=None, **setup)\n",
    "steps_to_convergence = policy_evaluation.run_to_convergence(max_iterations = 1000)\n",
    "print(f\"Convergence in {steps_to_convergence} iterations\")\n",
    "\n",
    "# show the final state values after convergence\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "info = {'text': policy_evaluation.end_values, 'precision': 0}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
