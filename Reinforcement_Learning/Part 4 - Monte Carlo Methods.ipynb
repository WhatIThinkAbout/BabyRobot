{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__[A Baby Robot's Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)__\n",
    "\n",
    "# Monte Carlo Methods\n",
    "## An Introduction to Reinforcement Learning: Part 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/part4/monte_carlo_gpi_visits_10x7_opt.gif\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Once again we're off to the casino, and this time it's situated in sunny Monte Carlo, made famous by its appearance in the classic movie _[Madagascar 3: Europe's Most Wanted](https://en.wikipedia.org/wiki/Madagascar_3:_Europe%27s_Most_Wanted)_ (although there's a slight chance that it was already famous).\n",
    "\n",
    "In our last visit to a casino we looked at the _[multi-armed bandit](https://medium.com/towards-data-science/multi-armed-bandits-part-1-b8d33ab80697)_ and used this as a way to visualise the problem of how to choose the best action when confronted with many possible actions.\n",
    "\n",
    "In terms of **Reinforcement Learning** the bandit problem can be thought of as representing a single state and the actions available within that state. _Monte Carlo_ methods extend this idea to cover multiple, interrelated, states.\n",
    "\n",
    "Additionally, in the previous problems we've looked at, we've always been given a full model of the environment. This model defines both the transition probabilities, that describe the chances of moving from one state to the next, and the reward received for making this transition.\n",
    "\n",
    "In _Monte Carlo_ methods this isn't the case. No model is given and instead the agent must discover the properties of the environment through exploration, gathering information as it moves from one state to the next. In other words, <i>Monte Carlo methods learn from experience.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/green_babyrobot_small.gif\"/></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "\n",
    "This notebook accompanies the Towards Data Science article and is part of _[A Baby Robot's Guide To Reinforcement Learning](https://towardsdatascience.com/tagged/baby-robot-guide)_\n",
    "\n",
    "\n",
    "To interactively run this notebook try opening it on Binder:\n",
    "\n",
    "[![Binder](https://mybinder.org/badge_logo.svg)](https://mybinder.org/v2/gh/WhatIThinkAbout/BabyRobot/HEAD?labpath=%2FReinforcement_Learning%2FPart%204%20-%20Monte%20Carlo%20Methods.ipynb)\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code Setup\n",
    "\n",
    "The examples in this notebook use the **[Baby Robot Custom Gym Environment](https://medium.com/towards-data-science/creating-a-custom-gym-environment-for-jupyter-notebooks-e17024474617)**.\n",
    "\n",
    "The source code for this can be found on _[Github](https://github.com/WhatIThinkAbout/BabyRobotGym)_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install Baby Robot Gym\n",
    "# %pip install --upgrade babyrobot -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baby Robot Version = 1.0.40\n"
     ]
    }
   ],
   "source": [
    "import babyrobot\n",
    "print(f\"Baby Robot Version = {babyrobot.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from copy import deepcopy\n",
    "from IPython.display import Image as PyImage\n",
    "\n",
    "from babyrobot.envs.lib import Actions\n",
    "from babyrobot.lib import Policy\n",
    "from babyrobot.lib import PolicyEvaluation\n",
    "from babyrobot.lib import Animate\n",
    "from babyrobot.lib import MonteCarloGPI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Monte Carlo Prediction\n",
    "\n",
    "In the prediction problem we want to find how good it is to be in a particular state of the environment. This \"_goodness_\" is represented by the state value, which is defined as the expected reward that can be obtained when starting in that state and then following the current policy for all subsequent states.\n",
    "\n",
    "When we have full knowledge about the environment, and know the transition probabilities and rewards, we can simply use _[Dynamic Programming](https://medium.com/towards-data-science/state-values-and-policy-evaluation-ceefdd8c2369#e996)_ to iteratively calculate the value for each state.\n",
    "\n",
    "In practice, its unlikely that a system's transition probabilities are known in advance. Therefore, to estimate how likely it is to move from one state to another, it's possible to observe multiple episodes and then take the average. This approach, of taking random samples to calculate estimates, is known as **_Monte Carlo Sampling_**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consider the level, shown in figure 1 below, where Baby Robot currently finds himself:\n",
    "\n",
    "<br><br>\n",
    "<center><img src=\"images/part4/glass_wall_level.png\"/></center>\n",
    "<center><i>Figure 1: A level containing a glass wall and the co-ordinates on this level.</i></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At first glance this level appears to be rather simple, with a short path from the start of the level to the exit. However, there are 2 obstacles of note:\n",
    "\n",
    "* In the top-middle square (coordinate (1,0)) there is a large puddle. As we've seen before, Baby Robot doesn't like puddles. They take longer to move through, incurring a negative reward of -4, and can cause him to skid.\\\n",
    "\\\n",
    "When a skid occurs Baby Robot won't reach the target state. Normally this would result in him moving to one of the other possible states, but in this case there are no other possible states, so he'll stay exactly where he is and receive another -4 penalty.\\\n",
    "\\\n",
    "If Baby Robot moves into this puddle there's a good chance he'll become stuck for several time periods and receive a large negative reward. It would be best to avoid this puddle!\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "\n",
    "* The thick blue line, between the cells (1,1) and (1,2), represents a glass wall. This is a new type of challenge that Baby Robot hasn't encountered before.\\\n",
    "\\\n",
    "Unlike standard walls, Baby Robot can't see glass walls and may therefore select an action that causes him to walk into the wall. When this happens he'll bounce off the wall and, rather than reaching the target state, end up in the opposite state. Also, he'll be given a negative reward penalty of -1 for the additional time required to make the move.\\\n",
    "\\\n",
    "In this level there are 2 possible opportunities for walking into the glass wall:\n",
    "\n",
    "  - if he moves South from cell (1,1) he'll instead end up in the puddle at (1,0) and receive a reward of -5 (-4 for moving into a puddle and -1 for hitting the glass wall).\n",
    "\n",
    "  - if he moves North from (1,2), instead of arriving at (1,1) he'll actually bounce off the wall and end up at the exit. In this case he'll be given a reward of -2 (-1 wall penalty and -1 for moving to a dry square)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Code Example\n",
    "\n",
    "The code below sets up and displays the example level, shown in Figure 1 above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "basesetup = {'width':3,'height':4,'start':[0,1],'end': [1,3]}\n",
    "\n",
    "# add a glass wall\n",
    "basesetup['walls'] = [((1, 1),'S',{'color':'#00a9ff','width':10,'fit':True,'prob':0.0})]\n",
    "basesetup['puddles'] = [((1,0),2)]   \n",
    "basesetup['base_areas'] = [(0,0,1,1),(0,2,1,2),(2,0,1,1),(2,3,1,1)]\n",
    "\n",
    "# use this if problems drawing puddles\n",
    "# basesetup['drawmode'] = 'colab'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfd12c0d78da4188b104a4bc387a8ac8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create the test environment using the base setup\n",
    "setup = deepcopy(basesetup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a setup for information display \n",
    "# - just show the graphical grid\n",
    "info_setup = deepcopy(basesetup)\n",
    "info_setup['show_start_text'] =False\n",
    "info_setup['show_end_text'] = False\n",
    "info_setup['robot'] = { 'show': False}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "182208b0f8034ccba972e06e6ada9c58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=296)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup = deepcopy(info_setup)\n",
    "setup['add_compass'] = True\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "info = {'coords': True}\n",
    "env.show_info(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "<br>\n",
    "\n",
    "As mentioned above, when we have complete information about a system, and know all of its transition probabilities and rewards, we can use _[Policy Evaluation](https://towardsdatascience.com/state-values-and-policy-evaluation-ceefdd8c2369)_ to calculate the state values.\n",
    "\n",
    "For this environment with a stochastic policy, that just chooses randomly between the available actions in each state, _Policy Evaluation_ gives the following state values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Convergence in 323 iterations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d865975cdca0494cb44899f13cc91579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# create a stochastic policy and a policy evaluation object for this policy\n",
    "policy = Policy(env)\n",
    "policy_evaluation = PolicyEvaluation( env, policy )\n",
    "\n",
    "setup = deepcopy(info_setup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", render_mode=None, **setup)\n",
    "steps_to_convergence = policy_evaluation.run_to_convergence(max_iterations = 1000)\n",
    "print(f\"Convergence in {steps_to_convergence} iterations\")\n",
    "\n",
    "# show the final state values after convergence\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "info = {'text': policy_evaluation.end_values, 'precision': 2}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Figure 2: State values calculated using Policy Evaluation for a stochastic policy.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of these state values represents the expected return from that state. So, at time '_t_' if we start in state '_s_' the value of a state under policy '_π_' is given by:\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/part4/equation1.png\"/></center>\n",
    "<center><i>Equation 1: The state value function under policy π</i></center>\n",
    "<br>\n",
    "\n",
    "Where the return '_Gₜ_', the total amount of reward accumulated over an episode, starting at time '_t_', is given by:\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/part4/equation1.png\"/></center>\n",
    "<center><i>Equation 2: The discount return at time 't'</i></center>\n",
    "<br>\n",
    "\n",
    "\n",
    "With a stochastic policy, for this simple level, we know that ultimately Baby Robot will reach the exit and the episode will terminate. Therefore we can set the discount factor '_γ_' to 1. Under these conditions the state value gives the average future total reward that can be expected when starting in that state.\n",
    "\n",
    "In other words, the value of a state is the total of all future rewards obtained from that state, until completion of an episode, averaged over infinite episodes.\n",
    "\n",
    "Therefore, to get a simple estimate of the state value, we can simply take the average return of multiple episodes that commence in that state. The more episodes we run the better our estimate will be. This is the exact approach taken by **_Monte Carlo methods_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c02ac35b77e4d95a6631354c3a8fa1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=260, sync_image_data=True, width=346), HBox(children=(Play(value=0, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from babyrobot.lib import Animate\n",
    "\n",
    "# helper function to display text information in the side panel\n",
    "def get_info_string( details ):\n",
    "  info = {}\n",
    "  info['side_info'] = \\\n",
    "  [\n",
    "    ((14,110),f\"Step: {details['step']}\"),\n",
    "    ((14,130),f\"Action: {details['action']}\"),\n",
    "    ((14,150),f\"New State: {details['new_state']}\"),\n",
    "    ((14,170),f\"Total Reward: {details['total_reward']}\"),\n",
    "  ]\n",
    "  return info\n",
    "\n",
    "# try changing these values to create different, random, paths\n",
    "seed = 1234\n",
    "max_steps = 5\n",
    "\n",
    "# create an environment with a stochastic policy\n",
    "setup = deepcopy(basesetup)\n",
    "setup['side_panel'] = {'width':150}\n",
    "setup['add_compass'] = True\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "\n",
    "# set the policy's random seed to create a reproducible trajectory\n",
    "policy = Policy(env,seed=seed)\n",
    "info = {'coords': True}\n",
    "env.show_info(info)\n",
    "\n",
    "# display default information about the episode\n",
    "args = { 'max_steps': max_steps }\n",
    "args['info_function'] = get_info_string\n",
    "animate = Animate(env)\n",
    "animate.show_policy(policy,**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Figure 3: A sample episode with a trajectory that goes directly from the start state to the exit.</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example, from the start state, if Baby Robot was lucky enough to follow a path that took him directly to the exit (as shown in _figure 3_ above), his trajectory would have the following states and rewards:\n",
    "\n",
    "```\n",
    "[([1,1], -1), ([2,1], -1), ([2,2], -1), ([1,2], -1), ([1,3], -1)]\n",
    "\n",
    "The moves on this trajectory are:\n",
    "\n",
    "[0,1]->[1,1] - reward = -1\n",
    "[1,1]->[2,1] - reward = -1\n",
    "[2,1]->[2,2] - reward = -1\n",
    "[2,2]->[1,2] - reward = -1\n",
    "[1,2]-> Exit - reward = -1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, for this episode, the return value is -5. If we then ran the experiment again, with a stochastic policy that produces a different, random, path through the level, we'd get a different return value. By adding the returns from many such episodes and taking the average we'd get an estimate of the state value for the start state (0,1). The more episodes we run, the closer this estimate would be to the true state value.\n",
    "\n",
    "For the initial state on this level, if we run multiple episodes and take the average return, we get an estimated state value of approximately -102, which is identical to the state value calculated using Policy Evaluation. \n",
    "\n",
    "However we've now calculated this state value purely by exploration, without knowing the model of the environment. Additionally, by following a random trajectory until termination of the episode, we've actually just performed our first **_Monte Carlo search_**.\n",
    "\n",
    "> _To see this in action, try changing the 'seed' and 'max_steps' parameters in the code above._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculating the return values for other states on the trajectory\n",
    "\n",
    "In the above example we saw that, by repeatedly running episodes from the start state of the level, we could produce an estimate of the state value for that state. However, each episode passes through many states before finally reaching the terminal state. Yet, in the very simple Monte Carlo search that we've just described, we only considered the return values obtained from the start state and ignored all the other information gathered during the episode. This is very inefficient.\n",
    "\n",
    "A better approach is to calculate the return value for every state that's seen during the episode. Then, over multiple random episodes, we can create an estimate for every state value that's been visited.\n",
    "\n",
    "As you can see for the sample trajectory given above (_figure 3_), on his way to the exit Baby Robot also visits the states [1,1],[2,1],[2,2] and [1,2]. So, during this episode, we can also gather information for all of these states. For each state visited, we can consider the future rewards obtained from each state to be the return for that state. This then gives us an estimated return value for all states seen during the trajectory, not just the start state.\n",
    "\n",
    "For this sample trajectory we get the following return values for the states that have been visited:\n",
    "\n",
    "```\n",
    "[0,1] - return = -5\n",
    "[1,1] - return = -4\n",
    "[2,1] - return = -3\n",
    "[2,2] - return = -2\n",
    "[1,2] - return = -1\n",
    "```\n",
    "\n",
    "If we then average these values over many episodes we'll get an increasingly accurate estimate of the state value for all states that have been visited."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First-Visit Monte Carlo\n",
    "\n",
    "When each state is visited only once during an episode, as in our example above, then it's easy to calculate the return for each state - you just add up all future rewards from that state. But what happens when a state is seen multiple times?\n",
    "\n",
    "The simplest approach to calculate the value for a state that can be visited repeatedly in a single episode is to simply take the rewards obtained from the first time the state is visited until the end of the episode. When these returns are averaged over multiple episodes we can build up an estimate of the state value for every state that's been visited. Unsurprisingly, this approach is known as **_First-Visit Monte Carlo_**.\n",
    "\n",
    "\n",
    "### Example:\n",
    "\n",
    "One possible path from the entrance to the exit of the sample level is shown in _Figure 4_ below:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53a2ba4123a2469ab768e4e0b7f57c7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=260, sync_image_data=True, width=346), HBox(children=(Play(value=0, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from babyrobot.lib import DeterministicPolicy\n",
    "\n",
    "# create an environment with a deterministic policy\n",
    "setup = deepcopy(basesetup)\n",
    "setup['side_panel'] = {'width':150}\n",
    "setup['add_compass'] = True\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "\n",
    "# set the policy's actions to create a reproducible trajectory\n",
    "actions = [Actions.East,Actions.East,Actions.South,Actions.North,Actions.South,Actions.West,Actions.South]    \n",
    "policy = DeterministicPolicy(env,actions)  \n",
    "info = {'coords': True}\n",
    "env.show_info(info)\n",
    "\n",
    "# display default information about the episode\n",
    "args = { 'max_steps': 7 }\n",
    "args['info_function'] = get_info_string\n",
    "animate = Animate(env)\n",
    "animate.show_policy(policy,**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<i>Figure 4: A trajectory with repeated state visits</i>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This path has the following steps and rewards:\n",
    "\n",
    "```\n",
    "1. [0,1]->[1,1] - reward = -1\n",
    "2. [1,1]->[2,1] - reward = -1\n",
    "3. [2,1]->[2,2] - reward = -1\n",
    "4. [2,2]->[2,1] - reward = -1\n",
    "5. [2,1]->[2,2] - reward = -1\n",
    "6. [2,2]->[1,2] - reward = -1\n",
    "7. [1,2]-> Exit - reward = -1\n",
    "```\n",
    "\n",
    "In this trajectory, rather than moving straight to the exit, on step 4 Baby Robot takes a step backwards to state [2,1]. After this he follows a direct path to the exit. As a result of this backwards step he visits states [2,1] and [2,2] twice.\n",
    "\n",
    "With First-Visit Monte Carlo we sum the rewards obtained from the first time a state is seen until the end of the episode, to give an estimate of that state's value. When a state that's been seen before is re-visited, its reward is still used as part of the return, but we don't consider this to be a new trajectory from that state.\n",
    "\n",
    "For this trajectory the first-visit return values are shown below. Note that we don't calculate returns from steps 5 and 6, for states that have already been seen:\n",
    "\n",
    "```\n",
    "1. [0,1] - return = -7\n",
    "2. [1,1] - return = -6\n",
    "3. [2,1] - return = -5\n",
    "4. [2,2] - return = -4\n",
    "5. [2,1]\n",
    "6. [2,2]\n",
    "7. [1,2] - return = -1\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The steps of the method we've described above are detailed in the following Python Code, for the First-Visit Monte Carlo algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_to_returns(state_rewards):\n",
    "  # work backwards to convert the rewards into returns  \n",
    "  γ = 1.0\n",
    "  G = 0\n",
    "  state_returns = []\n",
    "  for state, reward in reversed(state_rewards):\n",
    "    G = reward + γ*G      \n",
    "    state_returns.append((state,G))  \n",
    "\n",
    "  # put back into the order of states visited\n",
    "  state_returns.reverse()  \n",
    "  return state_returns\n",
    "  \n",
    "\n",
    "def single_episode(env):\n",
    "  # run a single episode to collect the reward at each state of the trajectory\n",
    "  state,info = env.reset()\n",
    "  state_rewards = []\n",
    "  total_reward = 0\n",
    "  terminated = False\n",
    "  while not terminated:\n",
    "    action = env.action_space.sample()\n",
    "    new_state, reward, terminated, truncated, info = env.step(action)\n",
    "    total_reward += reward\n",
    "    state_rewards.append((state,reward))\n",
    "    state = new_state\n",
    "  return state_rewards\n",
    "\n",
    "\n",
    "def get_first_visit_return(state_returns):\n",
    "  # find the value of the first visit to a state  \n",
    "  # V = np.zeros((env.height,env.width))\n",
    "  V = np.tile(np.nan, (env.height,env.width))\n",
    "\n",
    "  # the first time a state is seen store the return value\n",
    "  for s, G in state_returns:    \n",
    "    # test if this state hasn't yet been vistied (entry is still NaN)\n",
    "    # convert from state's x,y coordinate to numpy row,col\n",
    "    if np.isnan(V[s[1],s[0]]):\n",
    "      V[s[1],s[0]] = G  \n",
    "\n",
    "  # get the states that were visited by looking at those that aren't NaN\n",
    "  visits = np.array( np.isnan(V)==False, dtype=int) \n",
    "\n",
    "  # replace all NaNs with zero to get the return at each state\n",
    "  returns = np.nan_to_num(V, copy=False, nan=0.0 )    \n",
    "  \n",
    "  return returns, visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_first_visit(env, max_episodes = 1):\n",
    "\n",
    "  # keep a count of the visits to each state\n",
    "  visits = np.zeros((env.height,env.width))                                 # ❶\n",
    "\n",
    "  # the total returns for each state\n",
    "  returns = np.zeros((env.height,env.width))\n",
    "\n",
    "  for episode in tqdm(range(max_episodes)):                                 # ❷\n",
    "\n",
    "    state_rewards = single_episode(env)                                     # ❸\n",
    "    state_returns = rewards_to_returns(state_rewards)                       # ❹\n",
    "    episode_returns, episode_visits = get_first_visit_return(state_returns) # ❺\n",
    "\n",
    "    # add the episode returns to the total returns\n",
    "    returns += episode_returns\n",
    "\n",
    "    # increment the count of any states that have been visited\n",
    "    visits += episode_visits\n",
    "  \n",
    "  return np.divide(returns, visits, out=np.zeros_like(returns), where=visits!=0), visits  # ❻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<b>The points to note in the code above, for First-Visit Monte Carlo, are as follows:</b>\n",
    "\n",
    "1. We want to keep track of the episode returns and number of visits to each state, therefore we start by creating zero'd Numpy arrays for each of these. The array dimensions are set to the width and height of the environment.\n",
    "\n",
    "2. Run for multiple episodes to get the returns generated over multiple trajectories. Here the number of runs is defined by '_max_episodes_'.\n",
    "\n",
    "3. Get the rewards for a single episode. This returns a list of all the states visited and the rewards received between the start and end of the episode.\n",
    "\n",
    "4. Convert the rewards into returns. This is done by simply summing the rewards in reverse direction.\n",
    "\n",
    "5. Take the first visit return value from the episode and add this value to the '_returns_' array. Additionally, for every state visited, we increment the count of the total number of visits to that state (i.e. we count only the number of first-visits).\n",
    "\n",
    "6. Finally, after all episodes have completed, we divide the total first-visit rewards by the total number of visits, to get the average return for each state. \\\n",
    "Here we're using the Numpy divide function to avoid divide by zero errors for any states that have not been visited."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b3780dee1db0424a90a7d8a914aea1ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/10000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup = deepcopy(info_setup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", render_mode=None, **setup)\n",
    "state_values, visits = monte_carlo_first_visit(env,10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a003aa2d9cb14eecb17cf9898c05f039",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the state values calculated using First-Visit MC\n",
    "info_setup['side_panel'] = {'width':0}\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**info_setup)\n",
    "info = {'text': state_values, 'precision': 0}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2829660fb4fa4e6ba3b3b7bf1ba2b189",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = babyrobot.make(\"BabyRobot-v0\",**info_setup)\n",
    "info = {'text': visits, 'precision': 0}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/part4/mc_first_visit_state.png\"/></center>\n",
    "<center><i>Figure 5: First-Visit MC calculated state values and state first-visit count for 10000 episodes.</i></center>\n",
    "<br>\n",
    "\n",
    "The state values, calculated using First-Visit Monte Carlo, running for 10000 episodes on this level, are shown above. Comparing these values to the ones we calculated previously using Policy Evaluation (see _Figure 2_), we can see that, for this very simple level, the values are identical.\n",
    "\n",
    "This shows the power of Monte Carlo methods. By simply observing the results from a set of random episodes we've been able to calculate very good estimates of the state values and all without any knowledge of the underlying environment properties.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample Average Estimates\n",
    "\n",
    "In the example above, to calculate the mean state values, we kept track of the total return and total number of first visits to each state. When all the episodes were complete the total return was divided by the number of visits to get the estimated state values. However, since the total return is getting progressively larger this has the potential to cause problems.\n",
    "\n",
    "A better approach is to use a running mean where, as each new return value is obtained, we update the estimated state value based on the last estimate. Not only does this help avoid problems with storage and compute time, but it also lets us have an estimate of the state value at each step. We don't need to wait until the process has finished to compute the average state value.\n",
    "\n",
    "The formula for calculating the new state value, in terms of the previous estimate and the return obtained from the latest episode, is shown below (where '_n_' is the number of visits to the state):\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/part4/equation3.png\"/></center>\n",
    "<center><i>Equation 3: The estimated value of state 's' calculated in terms of the previous value and new return.</i></center>\n",
    "<br>\n",
    "\n",
    "For a proof of this equation check out the Bandits article, where we used a similar method to calculate a sample average estimate for the action value."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> In the example below we're using the \n",
    "_[MonteCarloStateValues](https://github.com/WhatIThinkAbout/BabyRobotGym/blob/main/babyrobot/lib/monte_carlo.py)_ function from the Baby Robot Gym. This uses _sample average estimates_ and returns a '_delta_' value showing how the estimated state value is changing at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delta 0.02808: 100%|██████████████████████████████████████████| 10000/10000 [01:27<00:00, 114.60it/s]\n"
     ]
    }
   ],
   "source": [
    "from babyrobot.lib import MonteCarloStateValues\n",
    "\n",
    "setup = deepcopy(info_setup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "policy = Policy(env)\n",
    "mc = MonteCarloStateValues(policy,**setup)\n",
    "avg_returns, visits, deltas = mc.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f188afbc59864efb8728b4e916b25b38",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the calculated state values\n",
    "env = mc.show_values()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "36d620e83e8242a79d75c1dcec8241ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the number of visits to each state\n",
    "env = mc.show_visits()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploring Starts\n",
    "\n",
    "The second image in _Figure 5_ shows the state first-visit count. This number represents the number of episodes in which the state was visited at least once, as opposed to the total number of visits to that state. It is also equal to the number of trajectories that passed through this state on the way to the exit.\n",
    "\n",
    "In this simple environment there's only a single route from the entrance to the exit (this is the direct path that we saw back in _Figure 2_). To reach the exit Baby Robot has to pass through each of the states on this path. Therefore, all of the states on this path have to be visited at least once during an episode and, as a result, all have 10,000 first-visits, equal to the total number of episodes.\n",
    "\n",
    "In contrast, the grid square containing the puddle (at grid location [1,0]) doesn't lie on the direct path to the exit. It's not necessary to visit this state while moving to the exit and therefore this state only gets visited when an action, chosen at random by the policy, causes Baby Robot to move there. Consequently, it was only visited in 8,737 of the 10,000 episodes. \n",
    "\n",
    "From Baby Robot's point of view this was a good thing, as it stopped him getting wet, but from the point of view of the accuracy of our state estimate it's not so good, since this state was sampled fewer times than the other states. In general, the most frequently visited states will have more accurate estimates of the states values than the seldom seen states.\n",
    "\n",
    "To compensate for this we can instead choose to begin each episode in a different state. In this way we can guarantee that all state value estimates have an equal accuracy. Additionally, if our policy resulted in some states never being visited we could start some episodes in these states and, in this way, estimate a value for these states.\n",
    "\n",
    "This technique, of beginning each episode in a different state chosen from the total state space, is referred to as **_Exploring Starts_**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delta 0.01272: 100%|██████████████████████████████████████████| 10000/10000 [01:00<00:00, 165.71it/s]\n"
     ]
    }
   ],
   "source": [
    "setup = deepcopy(info_setup)\n",
    "setup['exploring_starts'] = True\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "policy = Policy(env)\n",
    "mc = MonteCarloStateValues(policy,**setup)\n",
    "avg_returns, visits, deltas = mc.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d770f21ae4743d8bf87139e13b21ed8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the calculated state values\n",
    "env = mc.show_values()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f39b639977794c01bc46708719703aff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the number of visits to each state\n",
    "env = mc.show_visits()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><img src=\"images/part4/mc_fv_exploring_starts.png\"/></center>\n",
    "<center><i>Figure 6: First-Visit MC with Exploring Starts. Calculated state values and state first-visit count for 10000 episodes.</i></center>\n",
    "<br>\n",
    "\n",
    "The state values and first-visit counts when using _Exploring Starts_ for 10,000 episodes is shown in _Figure 6_ above. You can see that, in this case, the number of first-visits to most states has actually decreased. This is due to the average trajectory being shorter since, in most cases, the starting state will be closer to the exit. For a simple environment, such as this one, _exploring starts_ doesn't really give any benefit. Indeed, while _exploring starts_ can help balance the visits to each state it may not always be possible or even desirable.\n",
    "\n",
    "When, as in this case, we're simply running a simulation of an environment then its very easy to select where each episode will begin. If, on the other hand, this was an actual, real-life, maze that _Baby Robot_ was navigating then selecting the start state would be considerably more difficult and, for certain environments, might actually be impossible.\n",
    "\n",
    "Additionally, although the values calculated for each state should be of a similar level of accuracy, this may not always be beneficial. Without _exploring starts_ the states that are most frequently visited will have a higher level of accuracy than those that are rarely visited. This can be a good thing since we're concentrating our efforts on finding the state values for the most often seen states and not wasting time on those that would be seen infrequently or never visited. However, by not using _exploring starts_ we run the risk of missing states that would potentially give high levels of return. As is always the case with Reinforcement Learning, its a balancing act between exploration and exploitation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Every-Visit Monte Carlo\n",
    "\n",
    "So far, with both our experiments that began at the level's start point and with _exploring starts_, we were using _First-Visit Monte Carlo_, in which the returns were taken from the first time a state was visited until the end of the episode.\n",
    "\n",
    "However, during any episode, a state may be visited multiple times (for example, see the sample trajectory given in _figure 4_, where the states [2,1] and [2,2] are both visited twice). So another approach to calculating an estimate of the state value is to treat each visit to a state as the start of a separate trajectory and record the return from that visit until the episode terminates. This approach to estimating state values is known as Every-Visit Monte Carlo.\n",
    "\n",
    "The results obtained for our sample level, when recording every visit to a state, are shown in Figure 7 below. Although the test was run for the standard 10,000 episodes, by recording every visit to a state the actual number of visits recorded to each state are in most cases much higher.\n",
    "\n",
    "Both _First-Visit_ and _Every-Visit Monte Carlo_ are, given enough time, guaranteed to converge on the true state value. However, although _Every-Visit MC_ records more visits to each state, it's not clear that it actually gives better results than _First-Visit MC_. This is probably due to the information from the extra trajectories that are recorded by _Every-Visit MC_ already being included in the return obtained from the _First-Visit_ trajectory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delta 0.00475: 100%|██████████████████████████████████████████| 10000/10000 [01:28<00:00, 112.74it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Every-Visit, no Exploring Starts '''\n",
    "setup = deepcopy(info_setup)\n",
    "setup['every_visit'] = True\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "policy = Policy(env)\n",
    "mc = MonteCarloStateValues(policy,**setup)\n",
    "avg_returns, visits, deltas = mc.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7bd4b3522d4749b086ddaba2578ed174",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the calculated state values\n",
    "env = mc.show_values()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1534e1923ae24ee8a3fc453347e73ecb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the number of visits to each state\n",
    "env = mc.show_visits()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delta 0.04804: 100%|██████████████████████████████████████████| 10000/10000 [01:00<00:00, 165.04it/s]\n"
     ]
    }
   ],
   "source": [
    "''' Every-Visit with Exploring Starts '''\n",
    "setup = deepcopy(info_setup)\n",
    "setup['exploring_starts'] = True\n",
    "setup['every_visit'] = True\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "policy = Policy(env)\n",
    "mc = MonteCarloStateValues(policy,**setup)\n",
    "avg_returns, visits, deltas = mc.run(10000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a14cbe8c8494efdb4b80c4a239a4ee3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the calculated state values\n",
    "env = mc.show_values()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2685e0f395434c19913e644145509d78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the number of visits to each state\n",
    "env = mc.show_visits()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><img src=\"images/part4/mc_everyvisit.png\"/></center>\n",
    "<center><i>Figure 7: Every-Visit MC, calculated state values and state visit count for 10,000 episodes.</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Monte Carlo Control\n",
    "\n",
    "Using any of the Monte Carlo methods described above, we are able to generate a pretty good estimate for the actual state values of the level for any supplied policy. However, although this lets us see the relative goodness of any state with respect to the other states, this doesn't actually help us to navigate the level.\n",
    "\n",
    "When we have full information about state transitions and rewards, we can use _Dynamic Programming_ to turn the _Bellman Equations_ into a set of update rules that can calculate the state values under the current policy. From these state values we can then select the action that gives the maximum expected return.\n",
    "\n",
    "This is summarised by the following equation for greedy policy _π_ in state _s_:\n",
    "\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/part4/policy_update_v.png\"/></center>\n",
    "<center><i>Equation 4: Greedy policy update w.r.t. the state values.</i></center>\n",
    "<br>\n",
    "\n",
    "where:\n",
    "<br>\n",
    "* _p(s',r|s,a)_ is the probability of moving to the next state _s'_ and getting reward r when starting in state _s_ and taking action _a_.\n",
    "* _r_ is the reward received after taking this action.\n",
    "* _γ_ is the discount factor.\n",
    "* _V(s')_ is the value of the next state.\n",
    "\n",
    "<br>\n",
    "Unfortunately, since we're not given the model of the environment, we don't know _p(s',r|s,a)_ and therefore can't make use of the next state value _V(s')_. Since we're not given a model of the environment we don't know where we'll end up if we take a specific action, nor the reward we'll get for taking that action.\n",
    "\n",
    "As an illustration, consider the level that Baby Robot has been exploring, along with the first-visit state values that we calculated, shown again in _figure 8_ below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><img src=\"images/part4/mc_state_values_and_coords.png\"/></center>\n",
    "<center><i>Figure 8: State values and coordinates.</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When Baby Robot reaches the grid square [1,1], as shown above, if we simply act greedily with respect to the calculated state values, then the optimal action appears to be a move south, to grid square [1,2], since this is the adjacent state with the highest state value. However, since we don't have a model of the environment, we know nothing about the transition probabilities or rewards that would be received for taking this action.\n",
    "\n",
    "If Baby Robot was to take this action his probability of reaching the target state [1,2] is actually zero and instead he'd bounce off the glass wall and end up in the puddle at [1,0]. Additionally, he'd receive a large negative reward of -5 for taking this action. So, in reality, this wouldn't be a good action to choose.\n",
    "\n",
    "Therefore, for _Monte Carlo control_, rather than calculating the values of each state we need to calculate the action values. As a reminder, these values are known as **_[Q-Values](https://towardsdatascience.com/markov-decision-processes-and-bellman-equations-45234cce9d25)_** and represent the returns that can be expected from taking an individual action. We can then use these to find the optimal policy and navigate the best path through the maze.\n",
    "\n",
    "Once we know the state-action values (or approximations of them) then we can do policy improvement by acting greedily with respect to these values, simply choosing the action with the maximum value in each state. We don't need the model of the environment and we don't _bootstrap_ (i.e. no use is made of the value of the potential next state).\n",
    "\n",
    "This policy update is described by equation 5, where, for state '_s_', the greedy policy '_π_' simply chooses the action '_a_' with the maximum state-action value '_q_'.\n",
    "\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/part4/policy_update_q.png\"/></center>\n",
    "<center><i>Equation 5: Greedy policy update w.r.t. the maximum state-action value.</i></center>\n",
    "<br>\n",
    "\n",
    "\n",
    "The code to calculate the state-action values using Monte Carlo is almost identical to the code that was used to calculate the state values, except now, rather than considering visits to individual states, we're analysing the visits to the actions within those states. \n",
    "\n",
    "The Python code to calculate the state-action values using First-Visit Monte Carlo is shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewards_to_returns(rewards):\n",
    "    ''' work backwards to convert the rewards into returns '''\n",
    "    γ = 1.0\n",
    "    G = 0\n",
    "    returns = []\n",
    "    for state, action, reward in reversed(rewards):\n",
    "      G = γ*G + reward\n",
    "      returns.append((state,action,G))\n",
    "\n",
    "    # put back into the order of actions visited\n",
    "    returns.reverse()\n",
    "    return returns\n",
    "\n",
    "\n",
    "def single_episode(env,policy):\n",
    "    '''\n",
    "      run a single episode to collect the reward for each action of the trajectory\n",
    "    '''\n",
    "    state,info = env.reset()\n",
    "    action_rewards = []\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        # get the policy's action in the current state\n",
    "        action = policy.get_action(env.x,env.y)\n",
    "\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        action_rewards.append((state,action,reward))\n",
    "        state = new_state\n",
    "\n",
    "    return action_rewards\n",
    "\n",
    "\n",
    "def get_first_visit_return(returns):\n",
    "    ''' find the value of the first visit to a state-action '''\n",
    "        \n",
    "    # an array of NaNs for all state actions\n",
    "    V = np.tile(np.nan, (env.height,env.width,len(Actions)))\n",
    "\n",
    "    # process every state-action pair in the supplied set of returns\n",
    "    for s, a, G in returns:\n",
    "        \n",
    "        # test if this state-action hasn't yet been vistied (entry is still NaN)\n",
    "        # convert from state's x,y coordinate to numpy row,col\n",
    "        if np.isnan(V[s[1],s[0],a]):\n",
    "            V[s[1],s[0],a] = G\n",
    "\n",
    "    # get the state-actions that were visited by looking at those that aren't NaN\n",
    "    visits = np.array( np.isnan(V)==False, dtype=int) \n",
    "\n",
    "    # replace all NaNs with zero to get the return at each state\n",
    "    returns = np.nan_to_num(V, copy=False, nan=0.0 )    \n",
    "\n",
    "    return returns, visits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_fv_action_values(env, policy, max_episodes = 1):\n",
    "    ''' calculate state-action values using First-Visit Monte Carlo '''\n",
    "\n",
    "    # keep a count of the visits to each action\n",
    "    visits = np.zeros((env.height,env.width,len(Actions)))                             # ❶\n",
    "\n",
    "    # the average returns for each action\n",
    "    returns = np.zeros((env.height,env.width,len(Actions)))\n",
    "\n",
    "    for episode in tqdm(range(max_episodes)):                                          # ❷\n",
    "\n",
    "        action_rewards = single_episode(env,policy)                                    # ❸\n",
    "        action_returns = rewards_to_returns(action_rewards)                            # ❹\n",
    "        episode_returns, episode_visits = get_first_visit_return(action_returns)       # ❺\n",
    "\n",
    "        # add the episode returns to the total returns\n",
    "        returns += episode_returns\n",
    "\n",
    "        # increment the count of any states that have been visited\n",
    "        visits += episode_visits\n",
    "  \n",
    "    avg_returns = np.divide(returns,visits,out=np.zeros_like(returns),where=visits!=0) # ❻\n",
    "    return avg_returns, visits  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "If you compare this code with the Monte Carlo First-Visit code that we used to calculate the state values, you'll see that it's practically identical. The only real difference is that now, instead of using a 2-dimensional array of the states in the environment, we use a third dimension to allow the Q-values to be calculated.\n",
    "<br>\n",
    "<br>\n",
    "<b>The points to note in the code above, for First-Visit Monte Carlo, are as follows:</b>\n",
    "\n",
    "1. Over the course of all episodes, we want to keep track of the returns that were received after the first visit to each action and the number of episodes in which each action was visited (for First-Visit MC). Therefore we start by creating zero'd Numpy arrays for each of these. As before, the array dimensions are set to the width and height of the environment, but now a third dimension is added for the number of possible actions in each state.\n",
    "\n",
    "2. As in all Monte Carlo methods we want to sample the rewards obtained over multiple episodes.\n",
    "\n",
    "3. For each episode get the list of the state actions visited and the rewards received for each of these.\n",
    "\n",
    "4. Sum the rewards in reverse direction to calculate the returns for each action.\n",
    "\n",
    "5. Calculate the first-visit return value for the actions taken during the episode and then add this to the total returns. This updates the total return of every action seen during the episode and increments the count of the number of first visits for these actions.\n",
    "\n",
    "6. Finally, after all episodes have completed, divide the total first-visit rewards by the total number of visits, to get the estimated Q-Value for each state-action.\n",
    "\n",
    "<br>\n",
    "Running this code for 1000 episodes gives the following state-action values and action visit counts:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b20e54e1b4b748e8865182c9f02f8b68",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup = deepcopy(info_setup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", render_mode=None, **setup)\n",
    "policy = Policy(env)\n",
    "action_values, visits = monte_carlo_fv_action_values(env, policy, 1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef99209b7a1340cdb3fb1c9e494fa0f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the state values calculated using First-Visit MC\n",
    "info_setup['side_panel'] = {'width':0}\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**info_setup)\n",
    "info = {'values': action_values, 'precision': 0}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51e871b872da403294c70542e97b8291",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "info_setup['side_panel'] = {'width':0}\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**info_setup)\n",
    "info = {'values': visits, 'precision': 0}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we can calculate the state-action values using the _[MonteCarloActionValues](https://github.com/WhatIThinkAbout/BabyRobotGym/blob/main/babyrobot/lib/monte_carlo.py)_ function from the Baby Robot Gym which uses sample average estimates and returns a 'delta' value to show how the estimated action values are changing at each step:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delta 0.11668: 100%|████████████████████████████████████████████| 1000/1000 [00:08<00:00, 118.19it/s]\n"
     ]
    }
   ],
   "source": [
    "from babyrobot.lib import MonteCarloActionValues\n",
    "\n",
    "setup = deepcopy(info_setup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "policy = Policy(env)\n",
    "mc = MonteCarloActionValues(policy,**setup)\n",
    "avg_returns, visits, deltas = mc.run(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "70078e6001bb4c0db03985e4cb681738",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the calculated state values\n",
    "env = mc.show_values()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d794ee9185e4a018e0c3a5ddd06ebb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the number of visits to each state\n",
    "env = mc.show_visits()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><img src=\"images/part4/mc_control.png\"/></center>\n",
    "<center><i>Figure 9: Action values and counts calculated using Monte Carlo.</i></center>\n",
    "<br>\n",
    "\n",
    "In _Figure 9_, above we can see that the actions that must be taken to move from the entrance to the exit of the level all have been sampled 1000 times, equal to the number of episodes. Similarly, since Baby Robot is smart enough to not walk into standard walls, those actions are never taken and so have a zero visit count. Due to a stochastic policy being used, all other possible actions have been visited across the set of episodes.\n",
    "\n",
    "If we now act greedily with respect to these action values we get the following policy which, for this very simple level, is also the optimal policy. By following this, Baby Robot will reach the exit in the shortest possible time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33a3024afcb44dad9483cb4e630b4eeb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=260, sync_image_data=True, width=196)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directions = policy.calculate_greedy_directions(action_values)\n",
    "setup = deepcopy(basesetup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "info = {'directions': {'arrows':directions,'text':directions}}\n",
    "env.show_info(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><img src=\"images/part4/optimal_policy.png\"/></center>\n",
    "<center><i>Figure 10: The optimal policy for this level, calculated by acting greedily wrt the estimated action values.</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Generalised Policy Iteration (GPI)\n",
    "\n",
    "For the simple level that we've been using as an example, with a stochastic policy where, in any state, each action was equally likely to be chosen, we evaluated the policy using Monte Carlo to estimate the action values and then improved the policy by selecting the action with the maximum value in each state (i.e. we acted greedily with respect to the action values). \n",
    "\n",
    "This represents a single iteration of _[Generalised Policy Iteration](https://towardsdatascience.com/policy-and-value-iteration-78501afb41d2)_ (GPI) in which repeated evaluation and improvement steps move the policy ever closer to the optimal policy, as shown in _figure 11_. But now, rather than using _[Dynamic Programming](https://medium.com/@tinkertytonk/state-values-and-policy-evaluation-in-5-minutes-f3e00f3c1a50#5f4c)_ to evaluate each policy, we've approximated the value function using Monte Carlo.\n",
    "\n",
    "\n",
    "<br>\n",
    "<center><img src=\"images/part4/GPI.png\"/></center>\n",
    "<center><i>Figure 11: Generalised Policy Iteration (GPI).</i></center>\n",
    "<br>\n",
    "In the case of our simple level we reached the optimal policy after only a single iteration but, in practice, for more complex environments many iterations may be required. \n",
    "\n",
    "As an example, consider the more complex level and an associated deterministic policy shown below:\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the environment\n",
    "basesetup = {'width':6,'height':3}\n",
    "\n",
    "walls = [((0, 0),'E'),((4, 2),'E'),((2, 2),'E'),((3, 1),'E'),((1, 1),'E'),((2, 0),'E')]\n",
    "basesetup['walls'] = walls\n",
    "\n",
    "basesetup['grid'] = {'colors': {'zero_fg':'rgba(40,40,40,0.2)','zero_bg':'rgba(40,40,40,0.2)'}}\n",
    "\n",
    "# limit each episode to a maximum of 1000 steps\n",
    "basesetup['max_steps'] = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df2fd2844b0f45958d6d80f1c90c9a7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup = deepcopy(basesetup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "433c72ac1aa7485c8fe274a2f9892f6c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' a deterministic policy that doesn't reach the exit '''\n",
    "\n",
    "directions = np.array([[4, 2, 4, 4, 8, 8],\n",
    "                       [4, 8, 1, 8, 4, 1],\n",
    "                       [2, 1, 8, 2, 1, 0]])\n",
    "\n",
    "policy = Policy(env)\n",
    "policy.set_policy(directions) \n",
    "\n",
    "setup = deepcopy(basesetup)\n",
    "# setup['add_compass'] = True\n",
    "setup['show_start_text'] =False\n",
    "setup['robot'] = { 'show': False}\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "info = {'directions': {'arrows':directions,'text':directions}}\n",
    "env.show_info(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<center><img src=\"images/part4/complex_level.png\"/></center>\n",
    "<br>\n",
    "<center><img src=\"images/part4/complex_level_initial_policy.png\"/></center>\n",
    "<center><i>Figure 12: A slightly more complex level and a deterministic policy for this level</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can run a single iteration of Monte Carlo policy evaluation for this deterministic policy, which gives us the following state-action values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Delta 1001.00000: 100%|████████████████████████████████████████████████| 1/1 [00:00<00:00,  7.24it/s]\n"
     ]
    }
   ],
   "source": [
    "# a single iteration of Monte Carlo to calculate action values\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "mc = MonteCarloActionValues(policy,**setup)\n",
    "action_values, visits, deltas = mc.run(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e856e5a88bb49e991ac311aad59dc1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the calculated state-action values\n",
    "env = mc.show_values()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Figure 13: state-action values after a single iteration of MC policy evaluation for a deterministic policy</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "83297ea18fbb4dfea7186e8e3e82529f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the number of visits to each state action\n",
    "env = mc.show_visits()\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And, if we act greedily with respect to these values, we get the following updated policy:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "011deca8c6ba42acb592e59981641a62",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# act greedily wrt the estimated state-action values\n",
    "directions = policy.calculate_greedy_directions(action_values)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "info = {'directions': {'arrows':directions,'text':directions}}\n",
    "env.show_info(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We appear to have a problem! Not only has the majority of the level not been visited, but Baby Robot is stuck running in circles, never reaching the exit (and this would have continued for ever is we hadn't set a limit on the allowed length of an episode). \n",
    "\n",
    "Clearly, when using a deterministic policy, we also need to add some form of exploration, otherwise we'll never be able to estimate the state-action values for actions that haven't been visited and, as a result, we'd never be able to improve the policy. \n",
    "\n",
    "We could achieve this exploration by using the exploring starts idea that we saw for state values. In this case, rather than beginning each episode in a new state, we'd extend this to also cover all actions. However, outside of a simple simulation of the environment, this would be impractical. A better approach is to consider policies that have a non-zero chance of selecting all actions in each state.\n",
    "\n",
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Epsilon-Greedy Algorithm (ε-Greedy)\n",
    "\n",
    "When following a deterministic policy only a single action will be taken in each state. However, to allow us to choose the best actions we need to form estimates of the values for all actions, so need to add some form of exploration. \n",
    "\n",
    "One of the simplest, but highly effective, methods for achieving this is the **_[Epsilon-Greedy](https://medium.com/towards-data-science/bandit-algorithms-34fd7890cb18#0145)_** algorithm (which we saw previously when looking at Bandit problems). In this method, in any state, we normally choose the action with the maximum estimated action value, but with probability 'ε' (epsilon) an action is selected at random. \n",
    "\n",
    "Therefore, for the current state, we can select an action as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def single_episode(env,policy,epsilon = 0.1):\n",
    "    '''\n",
    "      run a single episode to collect the reward for each action of the trajectory\n",
    "      - by default take the action specified by the policy\n",
    "      - otherwise, with a probability 'ε' (Epsilon), choose a random action      \n",
    "    '''                \n",
    "    state,info = env.reset()\n",
    "    action_rewards = []\n",
    "    total_reward = 0\n",
    "    terminated = False\n",
    "    while not terminated:\n",
    "        \n",
    "        # probability of selecting a random action\n",
    "        p = np.random.random()\n",
    "\n",
    "        # if the probability is less than epsilon then a random action \n",
    "        # is chosen from the state's available actions\n",
    "        if p < epsilon:\n",
    "            action = env.action_space.sample()\n",
    "        else:                \n",
    "            # get the policy's action in the current state\n",
    "            action = policy.get_action(env.x,env.y)                    \n",
    "\n",
    "        new_state, reward, terminated, truncated, info = env.step(action)\n",
    "        total_reward += reward\n",
    "        action_rewards.append((state,action,reward))\n",
    "        state = new_state\n",
    "\n",
    "    return action_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_fv_action_values(env, policy, max_episodes = 1, epsilon = 0.1):\n",
    "    ''' calculate state-action values using First-Visit Monte Carlo '''\n",
    "\n",
    "    # keep a count of the visits to each action\n",
    "    visits = np.zeros((env.height,env.width,len(Actions)))                             # ❶\n",
    "\n",
    "    # the average returns for each action\n",
    "    returns = np.zeros((env.height,env.width,len(Actions)))\n",
    "\n",
    "    for episode in tqdm(range(max_episodes)):                                          # ❷\n",
    "\n",
    "        action_rewards = single_episode(env,policy,epsilon)                            # ❸\n",
    "        action_returns = rewards_to_returns(action_rewards)                            # ❹\n",
    "        episode_returns, episode_visits = get_first_visit_return(action_returns)       # ❺\n",
    "\n",
    "        # add the episode returns to the total returns\n",
    "        returns += episode_returns\n",
    "\n",
    "        # increment the count of any states that have been visited\n",
    "        visits += episode_visits\n",
    "  \n",
    "    avg_returns = np.divide(returns,visits,out=np.zeros_like(returns),where=visits!=0) # ❻\n",
    "    return avg_returns, visits "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, when following an epsilon-greedy policy, we can discover other states and actions that wouldn't be found under a purely deterministic policy. As a result, we gather a lot more information during an episode than we would if simply following the deterministic policy.\n",
    "\n",
    "For example, for the same deterministic policy that was used above, but with the addition of random actions, taken with probability 0.1 (so approximately every 10th action will be chosen at random), for a single episode, we get action values like the one shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc1fd5ee2b054e58b9e4e5fd379e85d9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e03752c0ec340589e89c056acb16541",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# set a seed that chooses actions that reach the exit\n",
    "np.random.seed(202)\n",
    "\n",
    "# remove the limit on the number of steps in an episode\n",
    "setup['max_steps'] = 2000\n",
    "env = babyrobot.make(\"BabyRobot-v0\", render_mode=None, **setup)\n",
    "action_values, visits = monte_carlo_fv_action_values(env, policy, max_episodes = 1, epsilon = 0.1)\n",
    "\n",
    "# show the number of first-visits to each action\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "info = {'values': visits, 'precision': 0}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15a3a22745bc4a38bb109ec14175d6a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# show the state action values calculated using First-Visit MC\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "info = {'values': action_values, 'precision': 0}\n",
    "env.show_info(info) \n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Figure 15: Action values for a single episode of Epsilon-Greedy (with the allowed number of steps in an episode increased to 2000)</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we act greedily with respect to these action values we get the following policy which, as you can see, has now defined actions based on a much larger set of states:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6f78c2b070f4e049e306bcae96127cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "directions = policy.calculate_greedy_directions(action_values)\n",
    "env = babyrobot.make(\"BabyRobot-v0\", **setup )\n",
    "info = {'directions': {'arrows':directions,'text':directions}}\n",
    "env.show_info(info)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Figure 16: Policy formed from acting greedily with respect to action values calculated using an Epsilon-Greedy policy.</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Now that we've got a means by which we can explore the environment, we can return to our initial problem of using GPI to find the optimal policy.\n",
    "\n",
    "In the example above, we only ran a single episode of Monte Carlo to evaluate the state values and then improved the policy based on these values. Instead, before the policy improvement step, we could have evaluated the policy over multiple episodes, to form a more accurate approximation of the action values. \n",
    "\n",
    "These two approaches are similar to _[Value Iteration](https://medium.com/towards-data-science/policy-and-value-iteration-78501afb41d2#29d6)_, were only a single step of policy evaluation is done before the policy is improved, and _[Policy Iteration](https://medium.com/towards-data-science/policy-and-value-iteration-78501afb41d2#bad9)_, in which the policy is evaluated to convergence before the policy improvement step. In either case, for Monte Carlo methods, we need to run complete episodes in the policy evaluation step, before policy improvement takes place."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "635e70de97724f8fa8978c9dcaecf9ad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/500 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "050dd6fb391741028cf0709e910b2b7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "\n",
    "directions = np.array([[4, 2, 4, 2, 2, 4],\n",
    "                       [4, 1, 2, 4, 1, 4],\n",
    "                       [2, 1, 8, 2, 1, 0]])\n",
    "\n",
    "policy = Policy(env)\n",
    "policy.set_policy(directions) \n",
    "\n",
    "# keep a count of the visits to each action\n",
    "visits = np.zeros((env.height,env.width,len(Actions)))                             # ❶\n",
    "\n",
    "# the average returns for each action\n",
    "returns = np.zeros((env.height,env.width,len(Actions)))\n",
    "\n",
    "max_episodes = 500\n",
    "for episode in tqdm(range(max_episodes)):                                          # ❷\n",
    "\n",
    "    action_rewards = single_episode(env,policy)                                    # ❸\n",
    "    action_returns = rewards_to_returns(action_rewards)                            # ❹\n",
    "    episode_returns, episode_visits = get_first_visit_return(action_returns)       # ❺\n",
    "\n",
    "    # add the episode returns to the total returns\n",
    "    returns += episode_returns\n",
    "\n",
    "    # increment the count of any states that have been visited\n",
    "    visits += episode_visits\n",
    "\n",
    "    avg_returns = np.divide(returns,visits,out=np.zeros_like(returns),where=visits!=0) # ❻\n",
    "    directions = policy.update_policy(avg_returns)\n",
    "    \n",
    "info = {'directions': {'arrows':directions,'text':directions}}\n",
    "env.show_info(info)\n",
    "env.render()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Figure 17: an initial deterministic policy that does reach the exit</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d3cee97062534fb084656c69cf6a0fb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MultiCanvas(height=196, sync_image_data=True, width=388)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' draw the environment '''\n",
    "setup = deepcopy(basesetup)\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "env.render()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "''' animate Baby Robot following the policy '''\n",
    "\n",
    "# initialize the environment\n",
    "env.reset()\n",
    "\n",
    "# move until the exit is found\n",
    "terminated = False\n",
    "while not terminated:  \n",
    "  \n",
    "  # get the policy's action in the current state\n",
    "  action = policy.get_action(env.x,env.y)   \n",
    "\n",
    "  # take the action and get the information from the environment\n",
    "  new_state, reward, terminated, truncated, info = env.step(action)\n",
    "  \n",
    "  # show the current position\n",
    "  env.render() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8aeac41c96bc4360aac66fff587723ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=388), HBox(children=(Play(value=0, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "''' following the policy using the animation class '''\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "args = { 'max_steps': 11 }\n",
    "animate = Animate(env)\n",
    "animate.show_policy(policy,**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "The deterministic policy, shown in _Figure 17_ above, defines a trajectory that would take Baby Robot from the start to the exit of the level. But, as you've probably noticed, it's less than optimal.\n",
    "\n",
    "Therefore, we'd like to improve this policy by running repeated rounds of MC policy evaluation and policy improvement to move us progressively closer to the optimal policy. With epsilon set to 0.5, to give us a high degree of exploration, and running a single episode of MC policy evaluation before each policy improvement, we get the action value estimates and associated policies shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0686ad2bc2a947c49569be6ee67531a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=538), HBox(children=(Play(value=0, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "setup = deepcopy(basesetup)\n",
    "setup['robot'] = { 'show': False}\n",
    "setup['show_start_text'] =False\n",
    "setup['max_steps'] = 2000\n",
    "setup['grid'] = {'colors': {'zero_fg':'rgba(40,40,40,0.2)','zero_bg':'rgba(40,40,40,0.2)'}}\n",
    "setup['side_panel'] = {'width':150}\n",
    "\n",
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "\n",
    "# start with a deterministic policy\n",
    "directions = np.array([[4, 2, 4, 2, 2, 4],\n",
    "                       [4, 1, 2, 4, 1, 4],\n",
    "                       [2, 1, 8, 2, 1, 0]])\n",
    "\n",
    "policy = Policy(env)\n",
    "policy.set_policy(directions) \n",
    "\n",
    "mcGPI = MonteCarloGPI(policy,evaluation_steps=1,epsilon=0.5,seed=45,**setup)\n",
    "\n",
    "animate = Animate(env)\n",
    "args = { 'max_steps': 350, 'precision': 0 }\n",
    "args['show_directions'] = False\n",
    "args['show_visits'] = False\n",
    "animate.show_monte_carlo_gpi(mcGPI,min_delta=0.04,**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "85140bd5bb744a6f865a0941b023edb0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(MultiCanvas(height=196, sync_image_data=True, width=538), HBox(children=(Play(value=0, interval…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = babyrobot.make(\"BabyRobot-v0\",**setup)\n",
    "policy = Policy(env)\n",
    "policy.set_policy(directions) \n",
    "\n",
    "mcGPI = MonteCarloGPI(policy,evaluation_steps=1,epsilon=0.5,seed=45,**setup)\n",
    "\n",
    "animate = Animate(env)\n",
    "args = { 'max_steps': 500, 'precision': 0 }\n",
    "args['show_directions'] = True\n",
    "animate.show_monte_carlo_gpi(mcGPI,min_delta=0.04,**args)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Figure 18:  Action value estimates and associated policy improvements when running GPI with epsilon = 0.5</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The delta value, shown at each iteration of GPI in _figure 18_ and in the graph below, represents the average difference between the state action values at the start and end of the iteration. As you can see, it initially begins with a high value and converges towards zero as the estimated action values  become better estimates of the true action values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(mcGPI.deltas)\n",
    "plt.title('GPI Action Value Change')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Delta')\n",
    "plt.savefig(\"gpi_action_value_graph.jpg\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><i>Figure 19: The change in the delta value, representing the mean difference of all action values between the start and end of each iteration</i></center>\n",
    "<br>\n",
    "\n",
    "In this run we've actually used the delta value to control the stopping of the GPI process (considering convergence to have been reached when delta is less than 0.04). The final policy is shown in figure 20 below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img src=\"images/part4/step_23.png\"/></center>\n",
    "<center><i>Figure 20: The policy formed at the final iteration of GPI, with a minimum delta of 0.04 used as the stopping criteria.</i></center>\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This final policy defines a much shorter route from the entrance to the exit of the level. Additionally, the single action value of the start state has reduced from -45 in the initial policy down to -27 in the final policy. Since this value represents the return that can be expected when taking this action and, in the case of this environment where every action incurs a penalty of -1 for taking that action, you can see that we've dramatically reduced the amount of time that it will take Baby Robot to reach the exit.\n",
    "\n",
    "However, things aren't perfect and this isn't the optimal policy for this level. For example, if you consider the bottom left-hand state, you can see that the final policy defines an action that would take Baby Robot North, when a shorter route to the exit would be to move East.\n",
    "\n",
    "The best action hasn't been found due to a lack of sampling of the actions in this state. If you look at the number of first-visits to each action, as shown in Figure 21 below, you can see that the North action was only seen 3 times across the 23 episodes used for policy evaluation.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<center><img src=\"images/part4/step_23_action_visits.png\"/></center>\n",
    "<center><i>Figure 21: The first-visit count of the actions visited during the 23 iterations of GPI.</i></center>\n",
    "<br>\n",
    "\n",
    "\n",
    "There are a few reasons why this action was hardly ever visited:\n",
    "\n",
    "* We defined a minimum delta value that stopped GPI before the optimal policy was found.\n",
    "<br>\n",
    "<br>\n",
    "* We only ran a single episode for each policy evaluation step. Increasing the number of episodes would increase the accuracy of the action value estimates and increase the number of visits to each action.\n",
    "<br>\n",
    "<br>\n",
    "* By default, Epsilon-Greedy follows the current deterministic policy. Although other actions are chosen at random to increase the exploration of the environment, remote states and actions are less likely to be visited. \n",
    "<br>\n",
    "\n",
    "As mentioned previously, the fact that this state and its actions have been visited infrequently may not be a bad thing. Rather than visiting all states and actions an equal number of times, by concentrating on those that lead from the start state to the exit, we've reduced the time required to locate the most optimal trajectory through the maze. As usual it's the old exploration\\exploitation trade-off.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary\n",
    "\n",
    "Monte Carlo methods use sampling to learn from experience. Over the course of multiple episodes they gradually build up an increasingly accurate picture of the environment. As a result, they can estimate state and action values without being given a model of the environment. \n",
    "\n",
    "During the policy evaluation step of Generalized Policy Iteration (GPI), by running complete episodes using the current policy, Monte Carlo methods can be used to form an average of the visited state-action values. \n",
    "\n",
    "To ensure that all states and actions are visited, exploration can be introduced in a number of ways, such as _exploring starts_, in which each new episode begins in a different state or action, or _epsilon-greedy_ policies where, by default, the action specified by the policy is taken, but with a probability '_epsilon_' a different, random, action is chosen.\n",
    "\n",
    "If the policy is then improved by acting greedily with respect to these estimated values, the policy improvement theorem guarantees that the new policy will be as good or better than the previous policy. By repeating these steps of policy evaluation and improvement, Monte Carlo methods can be used to find the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What's Next\n",
    "\n",
    "So far we've only considered '**_on-policy_**' methods, in which a single policy is evaluated and improved. Going forwards we'll take a look at '**_off-policy_**' methods which employ more than one policy. For example, one policy can be used to explore and gather information, and a second policy, which is the policy that's actually used to negotiate the environment, is then created from this information.\n",
    "\n",
    "We've also seen that unlike Dynamic Programming, Monte Carlo methods don't use the values from other states when updating their value estimates. In other words they don't 'bootstrap'. In the next article we'll look at **_Temporal-Difference (TD)_** learning which, like Monte Carlo, learns from experience but also employs bootstrapping when calculating its value estimates."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# References:\n",
    "\n",
    "_[\"Reinforcement Learning: An Introduction\"](http://www.incompleteideas.net/book/RLbook2020.pdf)_, Sutton & Barto (2018)\n",
    "<br>\n",
    "_[\"Lecture 4: Model-free Prediction\"](https://www.youtube.com/watch?v=PnHCvfgC_ZA)_, David Silver"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<center><img src=\"images/green_babyrobot_small.gif\"/></center>\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
